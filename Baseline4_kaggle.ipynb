{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba6fbe4-9cf5-4cbf-9633-c5fac6f7da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "import playground.optivarfuncs as of\n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "is_offline = False \n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan \n",
    "split_day = 435  \n",
    "import polars as pl\n",
    "# set the max columns to none\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8438bf-2692-4ef1-b736-7c49532a2ab3",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40267af-acb7-4c17-a905-b33659850ecd",
   "metadata": {},
   "source": [
    "## Settings and helper Functions\n",
    "There are 480 dates, 5 days a week or 96 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4218260-02b6-4520-b479-0f94415818d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:    \n",
    "    #take last 3 months worth? or roughly 12*5=60.  So we want from (480-60) to 480\n",
    "    # start_date=420\n",
    "\n",
    "    #take last 1 months worth? or roughly 4*5=20.  So we want from (480-20) to 480\n",
    "    start_date=460\n",
    "\n",
    "    #just a week for testing?\n",
    "    start_date=475\n",
    "    \n",
    "    #take last 1 months worth? or roughly 4*5=20.  So we want from (480-20) to 480\n",
    "    doTrainModel= True #if true, #need train and test sets\n",
    "    runOnKaggle=False #if true, then concat all datasets before calculating features for Kaggle data\n",
    "\n",
    "    use_subset_of_data=False\n",
    "\n",
    "from gc import collect;\n",
    "collect()\n",
    "# Tracking kernel memory usage:-  \n",
    "from os import path, walk, getpid;\n",
    "from psutil import Process;\n",
    "def GetMemUsage():\n",
    "    \"\"\"\n",
    "    This function defines the memory usage across the kernel. \n",
    "    Source-\n",
    "    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n",
    "    \"\"\";\n",
    "    \n",
    "    pid = getpid();\n",
    "    py = Process(pid);\n",
    "    memory_use = py.memory_info()[0] / 2. ** 30;\n",
    "    return f\"RAM usage = {memory_use :.4} GB\";\n",
    "\n",
    "def cleanup(df):\n",
    "    #delete df object\n",
    "    try:\n",
    "        del df\n",
    "        df=None\n",
    "    except:\n",
    "        pass\n",
    "    collect()\n",
    "    return GetMemUsage()\n",
    "\n",
    "\n",
    "#logging\n",
    "import logging\n",
    "# set up logging to file - see previous section for more details\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename='logg.log',\n",
    "                    filemode='w')\n",
    "# define a Handler which writes INFO messages or higher to the sys.stderr\n",
    "console = logging.StreamHandler()\n",
    "# add the handler to the root logger\n",
    "logging.getLogger().addHandler(console)\n",
    "logger=logging.getLogger()\n",
    "\n",
    "#use following to enable and disable\n",
    "# logger.disabled = True\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f862245-b182-4273-af6a-66076e708d46",
   "metadata": {
    "papermill": {
     "duration": 0.007269,
     "end_time": "2023-12-11T06:19:10.995858",
     "exception": false,
     "start_time": "2023-12-11T06:19:10.988589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parallel Triplet Imbalance Calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2f409dd-2326-46d5-af2d-48d32e0e71ec",
   "metadata": {
    "papermill": {
     "duration": 0.595412,
     "end_time": "2023-12-11T06:19:11.598699",
     "exception": false,
     "start_time": "2023-12-11T06:19:11.003287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4290b60b-5ab5-458c-9ed9-fe3803a14443",
   "metadata": {
    "papermill": {
     "duration": 0.007346,
     "end_time": "2023-12-11T06:19:11.613752",
     "exception": false,
     "start_time": "2023-12-11T06:19:11.606406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Generation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08f21a3d-a940-47cd-be05-dcb107467d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# from tqdm.auto import tqdm  # for notebooks\n",
    "\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for backfilling\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LastValueAllStocks:\n",
    "    numberstocks = 200\n",
    "    date_id = 0 #initialise to 0\n",
    "    all_near_prices = {i:0 for i in range(numberstocks)}\n",
    "    all_far_prices = {i:0 for i in range(numberstocks)}\n",
    "\n",
    "    def set_initial_prices(self,df):\n",
    "        #Set the initial near_and far price for the first entry for each stock in dataframe\n",
    "\n",
    "        #first get the very first entry for all stocks\n",
    "        earliest_date=df.date_id.min()\n",
    "        earliest_seconds_in_bucket=df[df.date_id==earliest_date].seconds_in_bucket.min()\n",
    "        indices=df.loc[((df.date_id==earliest_date) & (df.seconds_in_bucket==earliest_seconds_in_bucket)),:].index\n",
    " \n",
    "        #then loop through and set the near and far prices for each stock in dftmp\n",
    "        for index in indices:\n",
    "            stk=df[df.index==index].stock_id.values[0]       \n",
    "            df.loc[df.index==index,['near_price']]=self.all_near_prices[stk]\n",
    "            df.loc[df.index==index,['far_price']]=self.all_far_prices[stk]\n",
    "        return df\n",
    "\n",
    "    def save_last_prices(self,df):\n",
    "        #save the final values for near_and far price for each stock in df\n",
    "\n",
    "        #first get the very last entry for all stocks\n",
    "        latest_date=df.date_id.max()\n",
    "        latest_seconds_in_bucket=df[df.date_id==latest_date].seconds_in_bucket.max()\n",
    "        indices=df.loc[((df.date_id==latest_date) & (df.seconds_in_bucket==latest_seconds_in_bucket)),:].index\n",
    "\n",
    "        #then loop through and set the near and far prices for each stock in dftmp\n",
    "        for index in indices:\n",
    "            stk=df[df.index==index].stock_id.values[0] \n",
    "            self.all_near_prices[stk]= df.loc[df.index==index,['near_price']].values[0][0] \n",
    "            self.all_far_prices[stk]= df.loc[df.index==index,['far_price']].values[0][0]\n",
    "        \n",
    "class HandleNaNs:\n",
    "    def __init__(self):\n",
    "        self.last_value_all_stocks=LastValueAllStocks()\n",
    "\n",
    "    def fill_nans(self,df):\n",
    "\n",
    "        #sort to get stock ids all together, should be chunks of 200 rows\n",
    "        df.sort_values(by=['stock_id','date_id','seconds_in_bucket'],inplace=True)\n",
    "\n",
    "        df=self.last_value_all_stocks.set_initial_prices(df)\n",
    "        \n",
    "        #then do a forward interpolation\n",
    "        df.far_price=df.far_price.interpolate(method='linear')\n",
    "        df.near_price=df.near_price.interpolate(method='linear')\n",
    "\n",
    "        self.last_value_all_stocks.save_last_prices(df)\n",
    "        df.sort_values(by=['date_id','seconds_in_bucket','stock_id'],inplace=True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cff5b11d",
   "metadata": {
    "papermill": {
     "duration": 0.035825,
     "end_time": "2023-12-11T06:19:11.656942",
     "exception": false,
     "start_time": "2023-12-11T06:19:11.621117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    \n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    \n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    #V4 feature\n",
    "    for window in [3,5,10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    #V5 - rolling diff\n",
    "    # Convert from pandas to Polars\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    #Define the windows and columns for which you want to calculate the rolling statistics\n",
    "    windows = [3, 5, 10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    # prepare the operations for each column and window\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    # Loop over each window and column to create the rolling mean and std expressions\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    # Run the operations using Polars' lazy API\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    # Execute the lazy expressions and overwrite the pl_df variable\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    # Convert back to pandas if necessary\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n",
    "    \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "class gen_all_features():\n",
    "    def __init__(self,df=None):\n",
    "        #infer near and far prices\n",
    "        self.hn=HandleNaNs()\n",
    "        \n",
    "    def generate_all_features(self,df):\n",
    "        #infer near and far prices\n",
    "\n",
    "        df=self.hn.fill_nans(df)\n",
    "        \n",
    "        # Select relevant columns for feature generation\n",
    "        cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\"]]\n",
    "        df = df[cols]\n",
    "        \n",
    "        # Generate imbalance features\n",
    "        df = imbalance_features(df)\n",
    "        gc.collect() \n",
    "        df = other_features(df)\n",
    "        gc.collect()  \n",
    "        feature_name = [i for i in df.columns if i not in [\"row_id\", \"time_id\"]]\n",
    "        \n",
    "        return df[feature_name]\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04326afa-4bdf-4887-b11f-28f4d354dbb5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Data Loading and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3041840a-8cb4-43c0-868a-72eede03b998",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 679.35 MB\n",
      "Memory usage after optimization is: 304.71 MB\n",
      "Decreased by 55.15%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAM usage = 1.222 GB'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getdata():\n",
    "    if(CONFIG.runOnKaggle==True):\n",
    "        df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(\"./data/train.csv\")\n",
    "    \n",
    "    df = df.dropna(subset=[\"target\"])  #drop all rows with NaN in target\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "    \n",
    "df=getdata()  \n",
    "df=reduce_mem_usage(df)\n",
    "GetMemUsage()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233d85f7-a67e-40f5-90c0-82201e8ca4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df shape=(5237892, 17)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAM usage = 1.222 GB'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if(CONFIG.use_subset_of_data):\n",
    "    #just take the last 4 weeks\n",
    "    df=df[df.date_id>CONFIG.start_date]\n",
    "print(f'df shape={df.shape}')\n",
    "GetMemUsage() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f891285-a972-436b-9542-681fcb49b751",
   "metadata": {
    "papermill": {
     "duration": 0.007465,
     "end_time": "2023-12-11T06:19:11.700730",
     "exception": false,
     "start_time": "2023-12-11T06:19:11.693265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42cc7947-910d-40dc-b63c-4cbe7ac285f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAM usage = 1.446 GB'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanup_dataframes():\n",
    "    #cleanup existing dataframes\n",
    "    cleanup(df_train)\n",
    "    cleanup(df_valid)\n",
    "    # cleanup(df_test_feats)\n",
    "    cleanup(y_train)\n",
    "    cleanup(y_valid)\n",
    "    # cleanup(y_test_feats)\n",
    "\n",
    "def getDataSets(df):\n",
    "    if ( CONFIG.doTrainModel == True):\n",
    "        #just need a train and a valid set\n",
    "        return of.get2_DatasetAndTarget(df, dep_var='target', val_size=0.05,copy=False, verbose = False) \n",
    "df_train, df_valid, y_train, y_valid = getDataSets(df)\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7946d97b-e767-4921-b178-6f5f9177ba17",
   "metadata": {},
   "source": [
    "# Calculate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59227a87-7e65-46c3-b185-e16d762572ff",
   "metadata": {
    "papermill": {
     "duration": 0.021163,
     "end_time": "2023-12-11T06:19:11.685439",
     "exception": false,
     "start_time": "2023-12-11T06:19:11.664276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d6ab83a-70e5-42e1-bc97-096fec154fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 768 ms, sys: 15.4 ms, total: 783 ms\n",
      "Wall time: 782 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_global_stock_id_feats(df2):\n",
    "#first get the stats based on the training features\n",
    "    global_stock_id_feats = {\n",
    "            \"median_size\": df2.groupby(\"stock_id\")[\"bid_size\"].median() + df2.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "            \"std_size\": df2.groupby(\"stock_id\")[\"bid_size\"].std() + df2.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": df2.groupby(\"stock_id\")[\"bid_size\"].max() - df2.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": df2.groupby(\"stock_id\")[\"bid_price\"].median() + df2.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "            \"std_price\": df2.groupby(\"stock_id\")[\"bid_price\"].std() + df2.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": df2.groupby(\"stock_id\")[\"bid_price\"].max() - df2.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "    return global_stock_id_feats\n",
    "\n",
    "global_stock_id_feats=get_global_stock_id_feats(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "753ffeac-41e4-410c-b594-76cd89878b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build df Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 3771.41 MB\n",
      "Memory usage after optimization is: 3122.03 MB\n",
      "Decreased by 17.22%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAM usage = 4.215 GB'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleanup_dataframes()\n",
    "gaf=gen_all_features()\n",
    "df = gaf.generate_all_features(df)\n",
    "\n",
    "print(\"Build df Finished.\")\n",
    "df=reduce_mem_usage(df)\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1dc28b3-61f9-44c5-a222-57011b5247b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAM usage = 9.593 GB'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_valid, y_train, y_valid = getDataSets(df)\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "489ea9be-7efe-4740-ae29-2645d89447d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stock_id</th>\n",
       "      <th>date_id</th>\n",
       "      <th>seconds_in_bucket</th>\n",
       "      <th>imbalance_size</th>\n",
       "      <th>imbalance_buy_sell_flag</th>\n",
       "      <th>reference_price</th>\n",
       "      <th>matched_size</th>\n",
       "      <th>far_price</th>\n",
       "      <th>near_price</th>\n",
       "      <th>bid_price</th>\n",
       "      <th>bid_size</th>\n",
       "      <th>ask_price</th>\n",
       "      <th>ask_size</th>\n",
       "      <th>wap</th>\n",
       "      <th>volume</th>\n",
       "      <th>mid_price</th>\n",
       "      <th>liquidity_imbalance</th>\n",
       "      <th>matched_imbalance</th>\n",
       "      <th>size_imbalance</th>\n",
       "      <th>reference_price_far_price_imb</th>\n",
       "      <th>reference_price_near_price_imb</th>\n",
       "      <th>reference_price_ask_price_imb</th>\n",
       "      <th>reference_price_bid_price_imb</th>\n",
       "      <th>reference_price_wap_imb</th>\n",
       "      <th>far_price_near_price_imb</th>\n",
       "      <th>far_price_ask_price_imb</th>\n",
       "      <th>far_price_bid_price_imb</th>\n",
       "      <th>far_price_wap_imb</th>\n",
       "      <th>near_price_ask_price_imb</th>\n",
       "      <th>near_price_bid_price_imb</th>\n",
       "      <th>near_price_wap_imb</th>\n",
       "      <th>ask_price_bid_price_imb</th>\n",
       "      <th>ask_price_wap_imb</th>\n",
       "      <th>bid_price_wap_imb</th>\n",
       "      <th>ask_price_bid_price_wap_imb2</th>\n",
       "      <th>ask_price_bid_price_reference_price_imb2</th>\n",
       "      <th>ask_price_wap_reference_price_imb2</th>\n",
       "      <th>bid_price_wap_reference_price_imb2</th>\n",
       "      <th>matched_size_bid_size_ask_size_imb2</th>\n",
       "      <th>matched_size_bid_size_imbalance_size_imb2</th>\n",
       "      <th>matched_size_ask_size_imbalance_size_imb2</th>\n",
       "      <th>bid_size_ask_size_imbalance_size_imb2</th>\n",
       "      <th>stock_weights</th>\n",
       "      <th>weighted_wap</th>\n",
       "      <th>wap_momentum</th>\n",
       "      <th>imbalance_momentum</th>\n",
       "      <th>price_spread</th>\n",
       "      <th>spread_intensity</th>\n",
       "      <th>price_pressure</th>\n",
       "      <th>market_urgency</th>\n",
       "      <th>depth_pressure</th>\n",
       "      <th>spread_depth_ratio</th>\n",
       "      <th>mid_price_movement</th>\n",
       "      <th>micro_price</th>\n",
       "      <th>relative_spread</th>\n",
       "      <th>all_prices_mean</th>\n",
       "      <th>all_sizes_mean</th>\n",
       "      <th>all_prices_std</th>\n",
       "      <th>all_sizes_std</th>\n",
       "      <th>all_prices_skew</th>\n",
       "      <th>all_sizes_skew</th>\n",
       "      <th>all_prices_kurt</th>\n",
       "      <th>all_sizes_kurt</th>\n",
       "      <th>matched_size_shift_1</th>\n",
       "      <th>matched_size_ret_1</th>\n",
       "      <th>matched_size_shift_3</th>\n",
       "      <th>matched_size_ret_3</th>\n",
       "      <th>matched_size_shift_5</th>\n",
       "      <th>matched_size_ret_5</th>\n",
       "      <th>matched_size_shift_10</th>\n",
       "      <th>matched_size_ret_10</th>\n",
       "      <th>imbalance_size_shift_1</th>\n",
       "      <th>imbalance_size_ret_1</th>\n",
       "      <th>imbalance_size_shift_3</th>\n",
       "      <th>imbalance_size_ret_3</th>\n",
       "      <th>imbalance_size_shift_5</th>\n",
       "      <th>imbalance_size_ret_5</th>\n",
       "      <th>imbalance_size_shift_10</th>\n",
       "      <th>imbalance_size_ret_10</th>\n",
       "      <th>reference_price_shift_1</th>\n",
       "      <th>reference_price_ret_1</th>\n",
       "      <th>reference_price_shift_3</th>\n",
       "      <th>reference_price_ret_3</th>\n",
       "      <th>reference_price_shift_5</th>\n",
       "      <th>reference_price_ret_5</th>\n",
       "      <th>reference_price_shift_10</th>\n",
       "      <th>reference_price_ret_10</th>\n",
       "      <th>imbalance_buy_sell_flag_shift_1</th>\n",
       "      <th>imbalance_buy_sell_flag_ret_1</th>\n",
       "      <th>imbalance_buy_sell_flag_shift_3</th>\n",
       "      <th>imbalance_buy_sell_flag_ret_3</th>\n",
       "      <th>imbalance_buy_sell_flag_shift_5</th>\n",
       "      <th>imbalance_buy_sell_flag_ret_5</th>\n",
       "      <th>imbalance_buy_sell_flag_shift_10</th>\n",
       "      <th>imbalance_buy_sell_flag_ret_10</th>\n",
       "      <th>ask_price_diff_1</th>\n",
       "      <th>ask_price_diff_3</th>\n",
       "      <th>ask_price_diff_5</th>\n",
       "      <th>ask_price_diff_10</th>\n",
       "      <th>bid_price_diff_1</th>\n",
       "      <th>bid_price_diff_3</th>\n",
       "      <th>bid_price_diff_5</th>\n",
       "      <th>bid_price_diff_10</th>\n",
       "      <th>ask_size_diff_1</th>\n",
       "      <th>ask_size_diff_3</th>\n",
       "      <th>ask_size_diff_5</th>\n",
       "      <th>ask_size_diff_10</th>\n",
       "      <th>bid_size_diff_1</th>\n",
       "      <th>bid_size_diff_3</th>\n",
       "      <th>bid_size_diff_5</th>\n",
       "      <th>bid_size_diff_10</th>\n",
       "      <th>weighted_wap_diff_1</th>\n",
       "      <th>weighted_wap_diff_3</th>\n",
       "      <th>weighted_wap_diff_5</th>\n",
       "      <th>weighted_wap_diff_10</th>\n",
       "      <th>price_spread_diff_1</th>\n",
       "      <th>price_spread_diff_3</th>\n",
       "      <th>price_spread_diff_5</th>\n",
       "      <th>price_spread_diff_10</th>\n",
       "      <th>price_change_diff_3</th>\n",
       "      <th>size_change_diff_3</th>\n",
       "      <th>price_change_diff_5</th>\n",
       "      <th>size_change_diff_5</th>\n",
       "      <th>price_change_diff_10</th>\n",
       "      <th>size_change_diff_10</th>\n",
       "      <th>rolling_diff_ask_price_3</th>\n",
       "      <th>rolling_std_diff_ask_price_3</th>\n",
       "      <th>rolling_diff_bid_price_3</th>\n",
       "      <th>rolling_std_diff_bid_price_3</th>\n",
       "      <th>rolling_diff_ask_size_3</th>\n",
       "      <th>rolling_std_diff_ask_size_3</th>\n",
       "      <th>rolling_diff_bid_size_3</th>\n",
       "      <th>rolling_std_diff_bid_size_3</th>\n",
       "      <th>rolling_diff_ask_price_5</th>\n",
       "      <th>rolling_std_diff_ask_price_5</th>\n",
       "      <th>rolling_diff_bid_price_5</th>\n",
       "      <th>rolling_std_diff_bid_price_5</th>\n",
       "      <th>rolling_diff_ask_size_5</th>\n",
       "      <th>rolling_std_diff_ask_size_5</th>\n",
       "      <th>rolling_diff_bid_size_5</th>\n",
       "      <th>rolling_std_diff_bid_size_5</th>\n",
       "      <th>rolling_diff_ask_price_10</th>\n",
       "      <th>rolling_std_diff_ask_price_10</th>\n",
       "      <th>rolling_diff_bid_price_10</th>\n",
       "      <th>rolling_std_diff_bid_price_10</th>\n",
       "      <th>rolling_diff_ask_size_10</th>\n",
       "      <th>rolling_std_diff_ask_size_10</th>\n",
       "      <th>rolling_diff_bid_size_10</th>\n",
       "      <th>rolling_std_diff_bid_size_10</th>\n",
       "      <th>mid_price*volume</th>\n",
       "      <th>harmonic_imbalance</th>\n",
       "      <th>dow</th>\n",
       "      <th>seconds</th>\n",
       "      <th>minute</th>\n",
       "      <th>time_to_market_close</th>\n",
       "      <th>global_median_size</th>\n",
       "      <th>global_std_size</th>\n",
       "      <th>global_ptp_size</th>\n",
       "      <th>global_median_price</th>\n",
       "      <th>global_std_price</th>\n",
       "      <th>global_ptp_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.180603e+06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>13380277.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999812</td>\n",
       "      <td>60651.500000</td>\n",
       "      <td>1.000026</td>\n",
       "      <td>8493.030273</td>\n",
       "      <td>1.0</td>\n",
       "      <td>69144.531250</td>\n",
       "      <td>0.999919</td>\n",
       "      <td>0.754340</td>\n",
       "      <td>-0.615890</td>\n",
       "      <td>7.141326</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>-9.400536e-05</td>\n",
       "      <td>0.139683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.139683</td>\n",
       "      <td>NaN</td>\n",
       "      <td>255.370880</td>\n",
       "      <td>3.269177</td>\n",
       "      <td>3.215423</td>\n",
       "      <td>59.816772</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>680.587524</td>\n",
       "      <td>0.000161</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.094687e-09</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.666608</td>\n",
       "      <td>4.157506e+06</td>\n",
       "      <td>0.516353</td>\n",
       "      <td>6.324881e+06</td>\n",
       "      <td>-0.968246</td>\n",
       "      <td>1.695159</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>2.775961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14899.660156</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>40761.296875</td>\n",
       "      <td>127343.101562</td>\n",
       "      <td>5.898989e+06</td>\n",
       "      <td>1.999730</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.017414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666039e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>1642214.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999896</td>\n",
       "      <td>3233.040039</td>\n",
       "      <td>1.000660</td>\n",
       "      <td>20605.089844</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23838.128906</td>\n",
       "      <td>1.000278</td>\n",
       "      <td>-0.728751</td>\n",
       "      <td>-0.815787</td>\n",
       "      <td>0.156905</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000382</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000052</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>-5.200776e-05</td>\n",
       "      <td>6.344985</td>\n",
       "      <td>12816.000000</td>\n",
       "      <td>6.344985</td>\n",
       "      <td>1744.000000</td>\n",
       "      <td>93.345680</td>\n",
       "      <td>9.032276</td>\n",
       "      <td>10.107005</td>\n",
       "      <td>8.404243</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>127.277512</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.204751e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.666742</td>\n",
       "      <td>4.581641e+05</td>\n",
       "      <td>0.516456</td>\n",
       "      <td>7.927594e+05</td>\n",
       "      <td>-0.968245</td>\n",
       "      <td>1.949990</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>3.819817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5589.119629</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>25487.480469</td>\n",
       "      <td>65035.031250</td>\n",
       "      <td>5.116777e+05</td>\n",
       "      <td>1.999878</td>\n",
       "      <td>0.005619</td>\n",
       "      <td>0.029370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.028799e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999561</td>\n",
       "      <td>1819368.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999403</td>\n",
       "      <td>37956.000000</td>\n",
       "      <td>1.000298</td>\n",
       "      <td>18995.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>56951.000000</td>\n",
       "      <td>0.999851</td>\n",
       "      <td>0.332935</td>\n",
       "      <td>-0.714567</td>\n",
       "      <td>1.998210</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000369</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>-0.000220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000149</td>\n",
       "      <td>-2.985892e-04</td>\n",
       "      <td>0.499201</td>\n",
       "      <td>4.662142</td>\n",
       "      <td>0.678887</td>\n",
       "      <td>2.776772</td>\n",
       "      <td>93.951370</td>\n",
       "      <td>5.724238</td>\n",
       "      <td>5.341909</td>\n",
       "      <td>13.972041</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>NaN</td>\n",
       "      <td>271.084564</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.571567e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.666544</td>\n",
       "      <td>5.447998e+05</td>\n",
       "      <td>0.516303</td>\n",
       "      <td>8.595368e+05</td>\n",
       "      <td>-0.968244</td>\n",
       "      <td>1.869559</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>3.507308</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25319.107422</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>25736.900391</td>\n",
       "      <td>72926.539062</td>\n",
       "      <td>1.069838e+06</td>\n",
       "      <td>2.000176</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.051622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.191768e+07</td>\n",
       "      <td>-1</td>\n",
       "      <td>1.000171</td>\n",
       "      <td>18389746.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>2324.899902</td>\n",
       "      <td>1.000214</td>\n",
       "      <td>479032.406250</td>\n",
       "      <td>1.0</td>\n",
       "      <td>481357.312500</td>\n",
       "      <td>1.000106</td>\n",
       "      <td>-0.990340</td>\n",
       "      <td>-0.213547</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.000086</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>-5.066397e-07</td>\n",
       "      <td>239.466660</td>\n",
       "      <td>0.251127</td>\n",
       "      <td>0.250871</td>\n",
       "      <td>168.705887</td>\n",
       "      <td>37.571739</td>\n",
       "      <td>0.543170</td>\n",
       "      <td>0.565807</td>\n",
       "      <td>23.995132</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2562.229492</td>\n",
       "      <td>-0.000213</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.466411e-10</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>0.666731</td>\n",
       "      <td>7.697196e+06</td>\n",
       "      <td>0.516447</td>\n",
       "      <td>9.008441e+06</td>\n",
       "      <td>-0.968246</td>\n",
       "      <td>0.424923</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>-3.574718</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4627.342285</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>41473.500000</td>\n",
       "      <td>94332.937500</td>\n",
       "      <td>1.928848e+06</td>\n",
       "      <td>1.999974</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.018551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4.475500e+05</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.999532</td>\n",
       "      <td>17860614.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.999394</td>\n",
       "      <td>16485.539062</td>\n",
       "      <td>1.000016</td>\n",
       "      <td>434.100006</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16919.638672</td>\n",
       "      <td>0.999705</td>\n",
       "      <td>0.948687</td>\n",
       "      <td>-0.951109</td>\n",
       "      <td>37.976360</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>-3.030921e-04</td>\n",
       "      <td>0.026360</td>\n",
       "      <td>3.507559</td>\n",
       "      <td>0.034131</td>\n",
       "      <td>3.391793</td>\n",
       "      <td>1111.652100</td>\n",
       "      <td>40.395496</td>\n",
       "      <td>38.945301</td>\n",
       "      <td>26.855202</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>278.364655</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>3.676050e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.666490</td>\n",
       "      <td>4.581271e+06</td>\n",
       "      <td>0.516261</td>\n",
       "      <td>8.855317e+06</td>\n",
       "      <td>-0.968245</td>\n",
       "      <td>1.996737</td>\n",
       "      <td>-1.875</td>\n",
       "      <td>3.988887</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>845.924988</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>33656.980469</td>\n",
       "      <td>80714.015625</td>\n",
       "      <td>1.604066e+06</td>\n",
       "      <td>1.999830</td>\n",
       "      <td>0.003781</td>\n",
       "      <td>0.017379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stock_id  date_id  seconds_in_bucket  imbalance_size  \\\n",
       "0         0        0                  0    3.180603e+06   \n",
       "1         1        0                  0    1.666039e+05   \n",
       "2         2        0                  0    3.028799e+05   \n",
       "3         3        0                  0    1.191768e+07   \n",
       "4         4        0                  0    4.475500e+05   \n",
       "\n",
       "   imbalance_buy_sell_flag  reference_price  matched_size  far_price  \\\n",
       "0                        1         0.999812   13380277.00        0.0   \n",
       "1                       -1         0.999896    1642214.25        0.0   \n",
       "2                       -1         0.999561    1819368.00        0.0   \n",
       "3                       -1         1.000171   18389746.00        0.0   \n",
       "4                       -1         0.999532   17860614.00        0.0   \n",
       "\n",
       "   near_price  bid_price      bid_size  ask_price       ask_size  wap  \\\n",
       "0         0.0   0.999812  60651.500000   1.000026    8493.030273  1.0   \n",
       "1         0.0   0.999896   3233.040039   1.000660   20605.089844  1.0   \n",
       "2         0.0   0.999403  37956.000000   1.000298   18995.000000  1.0   \n",
       "3         0.0   0.999999   2324.899902   1.000214  479032.406250  1.0   \n",
       "4         0.0   0.999394  16485.539062   1.000016     434.100006  1.0   \n",
       "\n",
       "          volume  mid_price  liquidity_imbalance  matched_imbalance  \\\n",
       "0   69144.531250   0.999919             0.754340          -0.615890   \n",
       "1   23838.128906   1.000278            -0.728751          -0.815787   \n",
       "2   56951.000000   0.999851             0.332935          -0.714567   \n",
       "3  481357.312500   1.000106            -0.990340          -0.213547   \n",
       "4   16919.638672   0.999705             0.948687          -0.951109   \n",
       "\n",
       "   size_imbalance  reference_price_far_price_imb  \\\n",
       "0        7.141326                            1.0   \n",
       "1        0.156905                            1.0   \n",
       "2        1.998210                            1.0   \n",
       "3        0.004853                            1.0   \n",
       "4       37.976360                            1.0   \n",
       "\n",
       "   reference_price_near_price_imb  reference_price_ask_price_imb  \\\n",
       "0                             1.0                      -0.000107   \n",
       "1                             1.0                      -0.000382   \n",
       "2                             1.0                      -0.000369   \n",
       "3                             1.0                      -0.000022   \n",
       "4                             1.0                      -0.000242   \n",
       "\n",
       "   reference_price_bid_price_imb  reference_price_wap_imb  \\\n",
       "0                       0.000000                -0.000094   \n",
       "1                       0.000000                -0.000052   \n",
       "2                       0.000079                -0.000220   \n",
       "3                       0.000086                 0.000085   \n",
       "4                       0.000069                -0.000234   \n",
       "\n",
       "   far_price_near_price_imb  far_price_ask_price_imb  far_price_bid_price_imb  \\\n",
       "0                       NaN                     -1.0                     -1.0   \n",
       "1                       NaN                     -1.0                     -1.0   \n",
       "2                       NaN                     -1.0                     -1.0   \n",
       "3                       NaN                     -1.0                     -1.0   \n",
       "4                       NaN                     -1.0                     -1.0   \n",
       "\n",
       "   far_price_wap_imb  near_price_ask_price_imb  near_price_bid_price_imb  \\\n",
       "0               -1.0                      -1.0                      -1.0   \n",
       "1               -1.0                      -1.0                      -1.0   \n",
       "2               -1.0                      -1.0                      -1.0   \n",
       "3               -1.0                      -1.0                      -1.0   \n",
       "4               -1.0                      -1.0                      -1.0   \n",
       "\n",
       "   near_price_wap_imb  ask_price_bid_price_imb  ask_price_wap_imb  \\\n",
       "0                -1.0                 0.000107           0.000013   \n",
       "1                -1.0                 0.000382           0.000330   \n",
       "2                -1.0                 0.000448           0.000149   \n",
       "3                -1.0                 0.000107           0.000107   \n",
       "4                -1.0                 0.000311           0.000008   \n",
       "\n",
       "   bid_price_wap_imb  ask_price_bid_price_wap_imb2  \\\n",
       "0      -9.400536e-05                      0.139683   \n",
       "1      -5.200776e-05                      6.344985   \n",
       "2      -2.985892e-04                      0.499201   \n",
       "3      -5.066397e-07                    239.466660   \n",
       "4      -3.030921e-04                      0.026360   \n",
       "\n",
       "   ask_price_bid_price_reference_price_imb2  \\\n",
       "0                                       NaN   \n",
       "1                              12816.000000   \n",
       "2                                  4.662142   \n",
       "3                                  0.251127   \n",
       "4                                  3.507559   \n",
       "\n",
       "   ask_price_wap_reference_price_imb2  bid_price_wap_reference_price_imb2  \\\n",
       "0                            0.139683                                 NaN   \n",
       "1                            6.344985                         1744.000000   \n",
       "2                            0.678887                            2.776772   \n",
       "3                            0.250871                          168.705887   \n",
       "4                            0.034131                            3.391793   \n",
       "\n",
       "   matched_size_bid_size_ask_size_imb2  \\\n",
       "0                           255.370880   \n",
       "1                            93.345680   \n",
       "2                            93.951370   \n",
       "3                            37.571739   \n",
       "4                          1111.652100   \n",
       "\n",
       "   matched_size_bid_size_imbalance_size_imb2  \\\n",
       "0                                   3.269177   \n",
       "1                                   9.032276   \n",
       "2                                   5.724238   \n",
       "3                                   0.543170   \n",
       "4                                  40.395496   \n",
       "\n",
       "   matched_size_ask_size_imbalance_size_imb2  \\\n",
       "0                                   3.215423   \n",
       "1                                  10.107005   \n",
       "2                                   5.341909   \n",
       "3                                   0.565807   \n",
       "4                                  38.945301   \n",
       "\n",
       "   bid_size_ask_size_imbalance_size_imb2  stock_weights  weighted_wap  \\\n",
       "0                              59.816772          0.004         0.004   \n",
       "1                               8.404243          0.001         0.001   \n",
       "2                              13.972041          0.002         0.002   \n",
       "3                              23.995132          0.006         0.006   \n",
       "4                              26.855202          0.004         0.004   \n",
       "\n",
       "   wap_momentum  imbalance_momentum  price_spread  spread_intensity  \\\n",
       "0           NaN                 NaN      0.000214               NaN   \n",
       "1           NaN                 NaN      0.000764               NaN   \n",
       "2           NaN                 NaN      0.000895               NaN   \n",
       "3           NaN                 NaN      0.000215               NaN   \n",
       "4           NaN                 NaN      0.000622               NaN   \n",
       "\n",
       "   price_pressure  market_urgency  depth_pressure  spread_depth_ratio  \\\n",
       "0      680.587524        0.000161            -0.0        3.094687e-09   \n",
       "1      127.277512       -0.000557             0.0        3.204751e-08   \n",
       "2      271.084564        0.000298            -0.0        1.571567e-08   \n",
       "3     2562.229492       -0.000213             0.0        4.466411e-10   \n",
       "4      278.364655        0.000590            -0.0        3.676050e-08   \n",
       "\n",
       "   mid_price_movement  micro_price  relative_spread  all_prices_mean  \\\n",
       "0                   0          1.0         0.000214         0.666608   \n",
       "1                   0          1.0         0.000764         0.666742   \n",
       "2                   0          1.0         0.000895         0.666544   \n",
       "3                   0          1.0         0.000215         0.666731   \n",
       "4                   0          1.0         0.000622         0.666490   \n",
       "\n",
       "   all_sizes_mean  all_prices_std  all_sizes_std  all_prices_skew  \\\n",
       "0    4.157506e+06        0.516353   6.324881e+06        -0.968246   \n",
       "1    4.581641e+05        0.516456   7.927594e+05        -0.968245   \n",
       "2    5.447998e+05        0.516303   8.595368e+05        -0.968244   \n",
       "3    7.697196e+06        0.516447   9.008441e+06        -0.968246   \n",
       "4    4.581271e+06        0.516261   8.855317e+06        -0.968245   \n",
       "\n",
       "   all_sizes_skew  all_prices_kurt  all_sizes_kurt  matched_size_shift_1  \\\n",
       "0        1.695159           -1.875        2.775961                   NaN   \n",
       "1        1.949990           -1.875        3.819817                   NaN   \n",
       "2        1.869559           -1.875        3.507308                   NaN   \n",
       "3        0.424923           -1.875       -3.574718                   NaN   \n",
       "4        1.996737           -1.875        3.988887                   NaN   \n",
       "\n",
       "   matched_size_ret_1  matched_size_shift_3  matched_size_ret_3  \\\n",
       "0                 NaN                   NaN                 NaN   \n",
       "1                 NaN                   NaN                 NaN   \n",
       "2                 NaN                   NaN                 NaN   \n",
       "3                 NaN                   NaN                 NaN   \n",
       "4                 NaN                   NaN                 NaN   \n",
       "\n",
       "   matched_size_shift_5  matched_size_ret_5  matched_size_shift_10  \\\n",
       "0                   NaN                 NaN                    NaN   \n",
       "1                   NaN                 NaN                    NaN   \n",
       "2                   NaN                 NaN                    NaN   \n",
       "3                   NaN                 NaN                    NaN   \n",
       "4                   NaN                 NaN                    NaN   \n",
       "\n",
       "   matched_size_ret_10  imbalance_size_shift_1  imbalance_size_ret_1  \\\n",
       "0                  NaN                     NaN                   NaN   \n",
       "1                  NaN                     NaN                   NaN   \n",
       "2                  NaN                     NaN                   NaN   \n",
       "3                  NaN                     NaN                   NaN   \n",
       "4                  NaN                     NaN                   NaN   \n",
       "\n",
       "   imbalance_size_shift_3  imbalance_size_ret_3  imbalance_size_shift_5  \\\n",
       "0                     NaN                   NaN                     NaN   \n",
       "1                     NaN                   NaN                     NaN   \n",
       "2                     NaN                   NaN                     NaN   \n",
       "3                     NaN                   NaN                     NaN   \n",
       "4                     NaN                   NaN                     NaN   \n",
       "\n",
       "   imbalance_size_ret_5  imbalance_size_shift_10  imbalance_size_ret_10  \\\n",
       "0                   NaN                      NaN                    NaN   \n",
       "1                   NaN                      NaN                    NaN   \n",
       "2                   NaN                      NaN                    NaN   \n",
       "3                   NaN                      NaN                    NaN   \n",
       "4                   NaN                      NaN                    NaN   \n",
       "\n",
       "   reference_price_shift_1  reference_price_ret_1  reference_price_shift_3  \\\n",
       "0                      NaN                    NaN                      NaN   \n",
       "1                      NaN                    NaN                      NaN   \n",
       "2                      NaN                    NaN                      NaN   \n",
       "3                      NaN                    NaN                      NaN   \n",
       "4                      NaN                    NaN                      NaN   \n",
       "\n",
       "   reference_price_ret_3  reference_price_shift_5  reference_price_ret_5  \\\n",
       "0                    NaN                      NaN                    NaN   \n",
       "1                    NaN                      NaN                    NaN   \n",
       "2                    NaN                      NaN                    NaN   \n",
       "3                    NaN                      NaN                    NaN   \n",
       "4                    NaN                      NaN                    NaN   \n",
       "\n",
       "   reference_price_shift_10  reference_price_ret_10  \\\n",
       "0                       NaN                     NaN   \n",
       "1                       NaN                     NaN   \n",
       "2                       NaN                     NaN   \n",
       "3                       NaN                     NaN   \n",
       "4                       NaN                     NaN   \n",
       "\n",
       "   imbalance_buy_sell_flag_shift_1  imbalance_buy_sell_flag_ret_1  \\\n",
       "0                              NaN                            NaN   \n",
       "1                              NaN                            NaN   \n",
       "2                              NaN                            NaN   \n",
       "3                              NaN                            NaN   \n",
       "4                              NaN                            NaN   \n",
       "\n",
       "   imbalance_buy_sell_flag_shift_3  imbalance_buy_sell_flag_ret_3  \\\n",
       "0                              NaN                            NaN   \n",
       "1                              NaN                            NaN   \n",
       "2                              NaN                            NaN   \n",
       "3                              NaN                            NaN   \n",
       "4                              NaN                            NaN   \n",
       "\n",
       "   imbalance_buy_sell_flag_shift_5  imbalance_buy_sell_flag_ret_5  \\\n",
       "0                              NaN                            NaN   \n",
       "1                              NaN                            NaN   \n",
       "2                              NaN                            NaN   \n",
       "3                              NaN                            NaN   \n",
       "4                              NaN                            NaN   \n",
       "\n",
       "   imbalance_buy_sell_flag_shift_10  imbalance_buy_sell_flag_ret_10  \\\n",
       "0                               NaN                             NaN   \n",
       "1                               NaN                             NaN   \n",
       "2                               NaN                             NaN   \n",
       "3                               NaN                             NaN   \n",
       "4                               NaN                             NaN   \n",
       "\n",
       "   ask_price_diff_1  ask_price_diff_3  ask_price_diff_5  ask_price_diff_10  \\\n",
       "0               NaN               NaN               NaN                NaN   \n",
       "1               NaN               NaN               NaN                NaN   \n",
       "2               NaN               NaN               NaN                NaN   \n",
       "3               NaN               NaN               NaN                NaN   \n",
       "4               NaN               NaN               NaN                NaN   \n",
       "\n",
       "   bid_price_diff_1  bid_price_diff_3  bid_price_diff_5  bid_price_diff_10  \\\n",
       "0               NaN               NaN               NaN                NaN   \n",
       "1               NaN               NaN               NaN                NaN   \n",
       "2               NaN               NaN               NaN                NaN   \n",
       "3               NaN               NaN               NaN                NaN   \n",
       "4               NaN               NaN               NaN                NaN   \n",
       "\n",
       "   ask_size_diff_1  ask_size_diff_3  ask_size_diff_5  ask_size_diff_10  \\\n",
       "0              NaN              NaN              NaN               NaN   \n",
       "1              NaN              NaN              NaN               NaN   \n",
       "2              NaN              NaN              NaN               NaN   \n",
       "3              NaN              NaN              NaN               NaN   \n",
       "4              NaN              NaN              NaN               NaN   \n",
       "\n",
       "   bid_size_diff_1  bid_size_diff_3  bid_size_diff_5  bid_size_diff_10  \\\n",
       "0              NaN              NaN              NaN               NaN   \n",
       "1              NaN              NaN              NaN               NaN   \n",
       "2              NaN              NaN              NaN               NaN   \n",
       "3              NaN              NaN              NaN               NaN   \n",
       "4              NaN              NaN              NaN               NaN   \n",
       "\n",
       "   weighted_wap_diff_1  weighted_wap_diff_3  weighted_wap_diff_5  \\\n",
       "0                  NaN                  NaN                  NaN   \n",
       "1                  NaN                  NaN                  NaN   \n",
       "2                  NaN                  NaN                  NaN   \n",
       "3                  NaN                  NaN                  NaN   \n",
       "4                  NaN                  NaN                  NaN   \n",
       "\n",
       "   weighted_wap_diff_10  price_spread_diff_1  price_spread_diff_3  \\\n",
       "0                   NaN                  NaN                  NaN   \n",
       "1                   NaN                  NaN                  NaN   \n",
       "2                   NaN                  NaN                  NaN   \n",
       "3                   NaN                  NaN                  NaN   \n",
       "4                   NaN                  NaN                  NaN   \n",
       "\n",
       "   price_spread_diff_5  price_spread_diff_10  price_change_diff_3  \\\n",
       "0                  NaN                   NaN                  NaN   \n",
       "1                  NaN                   NaN                  NaN   \n",
       "2                  NaN                   NaN                  NaN   \n",
       "3                  NaN                   NaN                  NaN   \n",
       "4                  NaN                   NaN                  NaN   \n",
       "\n",
       "   size_change_diff_3  price_change_diff_5  size_change_diff_5  \\\n",
       "0                 NaN                  NaN                 NaN   \n",
       "1                 NaN                  NaN                 NaN   \n",
       "2                 NaN                  NaN                 NaN   \n",
       "3                 NaN                  NaN                 NaN   \n",
       "4                 NaN                  NaN                 NaN   \n",
       "\n",
       "   price_change_diff_10  size_change_diff_10  rolling_diff_ask_price_3  \\\n",
       "0                   NaN                  NaN                       NaN   \n",
       "1                   NaN                  NaN                       NaN   \n",
       "2                   NaN                  NaN                       NaN   \n",
       "3                   NaN                  NaN                       NaN   \n",
       "4                   NaN                  NaN                       NaN   \n",
       "\n",
       "   rolling_std_diff_ask_price_3  rolling_diff_bid_price_3  \\\n",
       "0                           NaN                       NaN   \n",
       "1                           NaN                       NaN   \n",
       "2                           NaN                       NaN   \n",
       "3                           NaN                       NaN   \n",
       "4                           NaN                       NaN   \n",
       "\n",
       "   rolling_std_diff_bid_price_3  rolling_diff_ask_size_3  \\\n",
       "0                           NaN                      NaN   \n",
       "1                           NaN                      NaN   \n",
       "2                           NaN                      NaN   \n",
       "3                           NaN                      NaN   \n",
       "4                           NaN                      NaN   \n",
       "\n",
       "   rolling_std_diff_ask_size_3  rolling_diff_bid_size_3  \\\n",
       "0                          NaN                      NaN   \n",
       "1                          NaN                      NaN   \n",
       "2                          NaN                      NaN   \n",
       "3                          NaN                      NaN   \n",
       "4                          NaN                      NaN   \n",
       "\n",
       "   rolling_std_diff_bid_size_3  rolling_diff_ask_price_5  \\\n",
       "0                          NaN                       NaN   \n",
       "1                          NaN                       NaN   \n",
       "2                          NaN                       NaN   \n",
       "3                          NaN                       NaN   \n",
       "4                          NaN                       NaN   \n",
       "\n",
       "   rolling_std_diff_ask_price_5  rolling_diff_bid_price_5  \\\n",
       "0                           NaN                       NaN   \n",
       "1                           NaN                       NaN   \n",
       "2                           NaN                       NaN   \n",
       "3                           NaN                       NaN   \n",
       "4                           NaN                       NaN   \n",
       "\n",
       "   rolling_std_diff_bid_price_5  rolling_diff_ask_size_5  \\\n",
       "0                           NaN                      NaN   \n",
       "1                           NaN                      NaN   \n",
       "2                           NaN                      NaN   \n",
       "3                           NaN                      NaN   \n",
       "4                           NaN                      NaN   \n",
       "\n",
       "   rolling_std_diff_ask_size_5  rolling_diff_bid_size_5  \\\n",
       "0                          NaN                      NaN   \n",
       "1                          NaN                      NaN   \n",
       "2                          NaN                      NaN   \n",
       "3                          NaN                      NaN   \n",
       "4                          NaN                      NaN   \n",
       "\n",
       "   rolling_std_diff_bid_size_5  rolling_diff_ask_price_10  \\\n",
       "0                          NaN                        NaN   \n",
       "1                          NaN                        NaN   \n",
       "2                          NaN                        NaN   \n",
       "3                          NaN                        NaN   \n",
       "4                          NaN                        NaN   \n",
       "\n",
       "   rolling_std_diff_ask_price_10  rolling_diff_bid_price_10  \\\n",
       "0                            NaN                        NaN   \n",
       "1                            NaN                        NaN   \n",
       "2                            NaN                        NaN   \n",
       "3                            NaN                        NaN   \n",
       "4                            NaN                        NaN   \n",
       "\n",
       "   rolling_std_diff_bid_price_10  rolling_diff_ask_size_10  \\\n",
       "0                            NaN                       NaN   \n",
       "1                            NaN                       NaN   \n",
       "2                            NaN                       NaN   \n",
       "3                            NaN                       NaN   \n",
       "4                            NaN                       NaN   \n",
       "\n",
       "   rolling_std_diff_ask_size_10  rolling_diff_bid_size_10  \\\n",
       "0                           NaN                       NaN   \n",
       "1                           NaN                       NaN   \n",
       "2                           NaN                       NaN   \n",
       "3                           NaN                       NaN   \n",
       "4                           NaN                       NaN   \n",
       "\n",
       "   rolling_std_diff_bid_size_10  mid_price*volume  harmonic_imbalance  dow  \\\n",
       "0                           NaN               0.0        14899.660156    0   \n",
       "1                           NaN               0.0         5589.119629    0   \n",
       "2                           NaN               0.0        25319.107422    0   \n",
       "3                           NaN               0.0         4627.342285    0   \n",
       "4                           NaN               0.0          845.924988    0   \n",
       "\n",
       "   seconds  minute  time_to_market_close  global_median_size  global_std_size  \\\n",
       "0        0       0                   540        40761.296875    127343.101562   \n",
       "1        0       0                   540        25487.480469     65035.031250   \n",
       "2        0       0                   540        25736.900391     72926.539062   \n",
       "3        0       0                   540        41473.500000     94332.937500   \n",
       "4        0       0                   540        33656.980469     80714.015625   \n",
       "\n",
       "   global_ptp_size  global_median_price  global_std_price  global_ptp_price  \n",
       "0     5.898989e+06             1.999730          0.003378          0.017414  \n",
       "1     5.116777e+05             1.999878          0.005619          0.029370  \n",
       "2     1.069838e+06             2.000176          0.005415          0.051622  \n",
       "3     1.928848e+06             1.999974          0.002912          0.018551  \n",
       "4     1.604066e+06             1.999830          0.003781          0.017379  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ad598a-e2a9-4b0d-88d8-73ac42a1ee17",
   "metadata": {},
   "source": [
    "# Reduce memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23f55203-46f6-4863-a2c0-fd9c476990db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 2977.05 MB\n",
      "Memory usage after optimization is: 2977.05 MB\n",
      "Decreased by 0.00%\n",
      "Memory usage of dataframe is 164.96 MB\n",
      "Memory usage after optimization is: 164.96 MB\n",
      "Decreased by 0.00%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAM usage = 9.593 GB'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = reduce_mem_usage(df_train)\n",
    "df_valid = reduce_mem_usage(df_valid)\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825ba59d-b7c7-4e82-b04a-d67a7a5ed5b9",
   "metadata": {},
   "source": [
    "# rolling?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2a42f24-5d54-4557-8669-850c7fe7b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.head()\n",
    "\n",
    "# df_train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_train.loc[df_train.stock_id==0,:].loc[:,['matched_size','matched_size_shift_1','matched_size_shift_3','matched_size_shift_5','matched_size_shift_10']].head(20)\n",
    "# test.loc[:,'bid_size']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6d3ff-2611-45dd-8058-bb6802d73d9a",
   "metadata": {},
   "source": [
    "# Save and Reload df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "852cd93c-ec1b-4bee-9815-a3272302a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_parquet(\"./tof_train.parquet\")\n",
    "df_valid.to_parquet(\"./tof_valid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2d9c7f9-1e26-41b9-9c12-b41e929f6fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAM usage = 17.39 GB'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_parquet(\"./tof_train.parquet\")\n",
    "df_valid=pd.read_parquet(\"./tof_valid.parquet\")\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3150fd21",
   "metadata": {},
   "source": [
    "# Split for the following clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "306b37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these come from months that were split into 4 clusters\n",
    "# c0=\"13  14  17  23  45  54  68  72  75  84  90  106  128  133  138  140  157  158  167  175  186\"\n",
    "# c0=[int(a) for a in c0.split('  ')]\n",
    "# c1=\"6  24  26  53  69  111  114  115  118  119  124  156  159  161  166  188  191  196  199\"\n",
    "# c1=[int(a) for a in c1.split('  ')]\n",
    "\n",
    "# #verify no intersection\n",
    "# print(f'intersection of c0 and c1={[a for a in c0 if a in c1]}')\n",
    "\n",
    "# #find all other stocks\n",
    "# allothers=[a for a in range(200) if a not in c0 and a not in c1]\n",
    "# clusters=[c0,c1,allothers]\n",
    "#run 13\n",
    "# clusters=[[0, 1, 28, 35, 50, 120, 121, 138, 153, 155, 167, 171, 179, 181, 25, 41, 47, 80, 84, 85, 113, 117, 133, 151, 175, 178, 186, 7, 37, 43, 51, 60, 139, 148, 165, 189, 23, 195, 44, 86, 180, 68, 81, 90, 36, 191, 131],\n",
    "# [2, 8, 40, 53, 55, 74, 77, 100, 101, 102, 114, 130, 150, 166, 177, 9, 18, 20, 29, 33, 52, 56, 59, 62, 99, 107, 111, 125, 149, 152, 196, 16, 39, 46, 135, 168, 170, 15, 63, 67, 79, 104, 145, 164, 173, 71, 93, 98, 119, 19, 136, 123, 161, 115, 82],\n",
    "# [3, 14, 32, 45, 48, 54, 58, 70, 78, 89, 94, 110, 141, 157, 176, 184, 13, 105, 160, 83, 198],\n",
    "# [4, 12, 22, 24, 27, 30, 31, 65, 96, 103, 158, 199, 5, 61, 64, 73, 75, 106, 112, 129, 154, 163, 169, 116, 192, 21, 128, 132, 134],\n",
    "# [6, 17, 42, 57, 88, 109, 118, 122, 126, 147, 159, 182, 188, 193, 10, 34, 49, 66, 69, 91, 97, 137, 142, 185, 194, 26, 92, 146, 174, 108, 197, 127, 76, 124, 72, 140, 87, 183, 162],\n",
    "# [11, 38, 95, 144, 172, 187, 190, 143, 156]]\n",
    "\n",
    "# allothers=[]\n",
    "# flattenclusters=[c for cluster in clusters for c in cluster]\n",
    "# allothers=[a for a in range(200) if a not in flattenclusters]\n",
    "# if allothers:\n",
    "#     clusters.append(allothers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b997d439-c59d-461b-89a7-4c209e6644c1",
   "metadata": {},
   "source": [
    "## Last optuna run for lightgbm gave these params\n",
    "\n",
    "lgb_params={'num_leaves': 244,\n",
    " 'learning_rate': 0.01,\n",
    " 'max_depth': 11,\n",
    " 'reg_alpha': 0.23746308577367767,\n",
    " 'reg_lambda': 2.702844990315888}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f319adb-7793-416b-8afc-591460104ecf",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de3adbc5-4632-4382-be10-a6f9d388988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mae(model, X_tst,y_tst):\n",
    "    y_pred = model.predict(X_tst)\n",
    "    return mean_absolute_error(y_pred, y_tst)\n",
    "\n",
    "def evaluate_simple(model, X_train, X_val,X_tst, y_train, y_val,y_tst):   \n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True),lgb.callback.log_evaluation(period=100)])\n",
    "    return get_mae(model, X_tst,y_tst)\n",
    "    \n",
    "def average_target(av_target_train, X_train, X_val, X_tst, y_train, y_val, y_tst):   \n",
    "    # model.fit(X_train, y_train, eval_set=[(X_val, y_val)], callbacks=[lgb.early_stopping(50, verbose=False)])\n",
    "    return len(y_tst), mean_absolute_error([av_target_train]*len(y_tst), y_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2062fcaf-2cbf-4d85-bada-878515c20ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_params={'num_leaves': 244,\n",
    "#  'learning_rate': 0.01,\n",
    "#  'max_depth': 11,\n",
    "#  'reg_alpha': 0.23746308577367767,\n",
    "#  'reg_lambda': 2.702844990315888}\n",
    "\n",
    "lgb_params = {\n",
    "        \"objective\": \"mae\",\n",
    "        \"n_estimators\": 6000,\n",
    "        \"num_leaves\": 256,\n",
    "        \"subsample\": 0.6,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "#         \"learning_rate\": 0.00871,\n",
    "        \"learning_rate\": 0.01,\n",
    "        'max_depth': 11,\n",
    "        \"n_jobs\": 4,\n",
    "        \"verbosity\": -1,\n",
    "        \"importance_type\": \"gain\",\n",
    "#         \"reg_alpha\": 0.1,\n",
    "        \"reg_alpha\": 0.2,\n",
    "        \"reg_lambda\": 3.25\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1bbe192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.73563\n",
      "[200]\tvalid_0's l1: 5.6754\n",
      "[300]\tvalid_0's l1: 5.65089\n",
      "[400]\tvalid_0's l1: 5.63677\n",
      "[500]\tvalid_0's l1: 5.62798\n",
      "[600]\tvalid_0's l1: 5.62175\n",
      "[700]\tvalid_0's l1: 5.61768\n",
      "[800]\tvalid_0's l1: 5.61449\n",
      "[900]\tvalid_0's l1: 5.61217\n",
      "[1000]\tvalid_0's l1: 5.61139\n",
      "[1100]\tvalid_0's l1: 5.61095\n",
      "[1200]\tvalid_0's l1: 5.61044\n",
      "[1300]\tvalid_0's l1: 5.60989\n",
      "[1400]\tvalid_0's l1: 5.60939\n",
      "[1500]\tvalid_0's l1: 5.60933\n",
      "[1600]\tvalid_0's l1: 5.60875\n",
      "[1700]\tvalid_0's l1: 5.60896\n",
      "Early stopping, best iteration is:\n",
      "[1617]\tvalid_0's l1: 5.60872\n",
      "Model for all data saved to models/m_0.txt\n",
      "CPU times: user 1h 59min 22s, sys: 3.7 s, total: 1h 59min 25s\n",
      "Wall time: 30min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "model_save_path = 'models' \n",
    "if not os.path.exists(model_save_path):\n",
    "    os.makedirs(model_save_path)\n",
    "\n",
    "models=[]\n",
    "# train 1 model on all the data, it will be the first model\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "model.fit(df_train, y_train, eval_set=[(df_valid, y_valid)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True),lgb.callback.log_evaluation(period=100)])\n",
    "models.append(model)\n",
    "\n",
    "# Save the model to a file\n",
    "model_filename = os.path.join(model_save_path, f'm_0.txt')\n",
    "model.booster_.save_model(model_filename)\n",
    "print(f\"Model for all data saved to {model_filename}\")\n",
    "\n",
    "\n",
    "# for i,cluster in enumerate(clusters):\n",
    "#     #split dataframe\n",
    "#     t=df_train.loc[df_train.stock_id.isin(cluster),:]\n",
    "#     v=df_valid.loc[df_valid.stock_id.isin(cluster),:]\n",
    "  \n",
    "#     # pull from y_train using t index\n",
    "#     y_t = y_train.loc[t.index]\n",
    "#     y_v = y_valid.loc[v.index]\n",
    "    \n",
    "#     # continue with the rest of your code\n",
    "#     model = lgb.LGBMRegressor(**lgb_params)\n",
    "#     model.fit(t, y_t, eval_set=[(v, y_v)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True),lgb.callback.log_evaluation(period=100)])\n",
    "    \n",
    "#     model_filename = os.path.join(model_save_path, f'm_{i+1}.txt')\n",
    "#     model.booster_.save_model(model_filename)\n",
    "#     print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "    \n",
    "#     models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "054dc736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.89449\n",
      "[200]\tvalid_0's l1: 5.87543\n",
      "[300]\tvalid_0's l1: 5.86448\n",
      "[400]\tvalid_0's l1: 5.85611\n",
      "[500]\tvalid_0's l1: 5.85211\n",
      "[600]\tvalid_0's l1: 5.84895\n",
      "[700]\tvalid_0's l1: 5.84654\n",
      "[800]\tvalid_0's l1: 5.84487\n",
      "[900]\tvalid_0's l1: 5.84396\n",
      "[1000]\tvalid_0's l1: 5.84329\n",
      "[1100]\tvalid_0's l1: 5.84288\n",
      "[1200]\tvalid_0's l1: 5.84232\n",
      "[1300]\tvalid_0's l1: 5.84201\n",
      "[1400]\tvalid_0's l1: 5.84224\n",
      "Early stopping, best iteration is:\n",
      "[1314]\tvalid_0's l1: 5.84193\n",
      "Model for all data saved to models/m_0.txt\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.56599\n",
      "[200]\tvalid_0's l1: 5.55264\n",
      "[300]\tvalid_0's l1: 5.54495\n",
      "[400]\tvalid_0's l1: 5.54208\n",
      "[500]\tvalid_0's l1: 5.54106\n",
      "[600]\tvalid_0's l1: 5.53978\n",
      "[700]\tvalid_0's l1: 5.53952\n",
      "Early stopping, best iteration is:\n",
      "[685]\tvalid_0's l1: 5.53907\n",
      "Model for fold 1 saved to models/m_1.txt\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.1646\n",
      "[200]\tvalid_0's l1: 6.14178\n",
      "[300]\tvalid_0's l1: 6.13172\n",
      "[400]\tvalid_0's l1: 6.12649\n",
      "[500]\tvalid_0's l1: 6.12181\n",
      "[600]\tvalid_0's l1: 6.11943\n",
      "[700]\tvalid_0's l1: 6.11706\n",
      "[800]\tvalid_0's l1: 6.11519\n",
      "[900]\tvalid_0's l1: 6.11419\n",
      "[1000]\tvalid_0's l1: 6.11345\n",
      "[1100]\tvalid_0's l1: 6.11209\n",
      "[1200]\tvalid_0's l1: 6.11206\n",
      "[1300]\tvalid_0's l1: 6.11189\n",
      "[1400]\tvalid_0's l1: 6.11207\n",
      "Early stopping, best iteration is:\n",
      "[1319]\tvalid_0's l1: 6.11177\n",
      "Model for fold 2 saved to models/m_2.txt\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 5.40751\n",
      "[200]\tvalid_0's l1: 5.39907\n",
      "[300]\tvalid_0's l1: 5.3967\n",
      "[400]\tvalid_0's l1: 5.39448\n",
      "Early stopping, best iteration is:\n",
      "[380]\tvalid_0's l1: 5.39329\n",
      "Model for fold 3 saved to models/m_3.txt\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.05583\n",
      "[200]\tvalid_0's l1: 6.04496\n",
      "[300]\tvalid_0's l1: 6.03634\n",
      "[400]\tvalid_0's l1: 6.03368\n",
      "[500]\tvalid_0's l1: 6.03372\n",
      "Early stopping, best iteration is:\n",
      "[428]\tvalid_0's l1: 6.03306\n",
      "Model for fold 4 saved to models/m_4.txt\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.01427\n",
      "[200]\tvalid_0's l1: 5.99305\n",
      "[300]\tvalid_0's l1: 5.98282\n",
      "[400]\tvalid_0's l1: 5.97845\n",
      "[500]\tvalid_0's l1: 5.97605\n",
      "[600]\tvalid_0's l1: 5.97612\n",
      "[700]\tvalid_0's l1: 5.97485\n",
      "[800]\tvalid_0's l1: 5.9746\n",
      "[900]\tvalid_0's l1: 5.97418\n",
      "Early stopping, best iteration is:\n",
      "[876]\tvalid_0's l1: 5.97404\n",
      "Model for fold 5 saved to models/m_5.txt\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\tvalid_0's l1: 6.15393\n",
      "[200]\tvalid_0's l1: 6.14636\n",
      "[300]\tvalid_0's l1: 6.1487\n",
      "Early stopping, best iteration is:\n",
      "[248]\tvalid_0's l1: 6.146\n",
      "Model for fold 6 saved to models/m_6.txt\n",
      "CPU times: user 2h 49min 40s, sys: 7.06 s, total: 2h 49min 47s\n",
      "Wall time: 42min 57s\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "# import os\n",
    "# model_save_path = 'models' \n",
    "# if not os.path.exists(model_save_path):\n",
    "#     os.makedirs(model_save_path)\n",
    "\n",
    "# models=[]\n",
    "# # train 1 model on all the data, it will be the first model\n",
    "# model = lgb.LGBMRegressor(**lgb_params)\n",
    "# model.fit(df_train, y_train, eval_set=[(df_valid, y_valid)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True),lgb.callback.log_evaluation(period=100)])\n",
    "# models.append(model)\n",
    "\n",
    "# # Save the model to a file\n",
    "# model_filename = os.path.join(model_save_path, f'm_0.txt')\n",
    "# model.booster_.save_model(model_filename)\n",
    "# print(f\"Model for all data saved to {model_filename}\")\n",
    "\n",
    "\n",
    "# for i,cluster in enumerate(clusters):\n",
    "#     #split dataframe\n",
    "#     t=df_train.loc[df_train.stock_id.isin(cluster),:]\n",
    "#     v=df_valid.loc[df_valid.stock_id.isin(cluster),:]\n",
    "  \n",
    "#     # pull from y_train using t index\n",
    "#     y_t = y_train.loc[t.index]\n",
    "#     y_v = y_valid.loc[v.index]\n",
    "    \n",
    "#     # continue with the rest of your code\n",
    "#     model = lgb.LGBMRegressor(**lgb_params)\n",
    "#     model.fit(t, y_t, eval_set=[(v, y_v)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True),lgb.callback.log_evaluation(period=100)])\n",
    "    \n",
    "#     model_filename = os.path.join(model_save_path, f'm_{i+1}.txt')\n",
    "#     model.booster_.save_model(model_filename)\n",
    "#     print(f\"Model for fold {i+1} saved to {model_filename}\")\n",
    "    \n",
    "#     models.append(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2009914e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         -3.029704\n",
       "1         -5.519986\n",
       "2         -8.389950\n",
       "3         -4.010201\n",
       "4         -7.349849\n",
       "             ...   \n",
       "5237887    2.310276\n",
       "5237888   -8.220077\n",
       "5237889    1.169443\n",
       "5237890   -1.540184\n",
       "5237891   -6.530285\n",
       "Name: target, Length: 5237892, dtype: float32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d227c1",
   "metadata": {},
   "source": [
    "# train all models again on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7714ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # del y_train\n",
    "# # del \n",
    "# #load the models\n",
    "\n",
    "# #get number of models\n",
    "# num_models=len(clusters)+1\n",
    "\n",
    "# models=[]\n",
    "# for i in range(num_models):\n",
    "#     model_filename = os.path.join(model_save_path, f'm_{i}.txt')\n",
    "#     model = lgb.Booster(model_file=model_filename)\n",
    "#     models.append(model)\n",
    "\n",
    "# # %%time\n",
    "# # # Train a LightGBM model for the current fold\n",
    "# # model = lgb.LGBMRegressor(**lgb_params)\n",
    "# # model.fit(df_train, y_train, eval_set=[(df_valid, y_valid)], callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=True),lgb.callback.log_evaluation(period=100)])\n",
    "\n",
    "# # GetMemUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b4cc2",
   "metadata": {},
   "source": [
    "# Lets see how they do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6325480d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.60872247403068"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds=mod.predict(df_valid)\n",
    "mean_absolute_error(preds, y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1915ba6e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstock_id \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;66;03m#in this case take the average of the first model and the one that was trained\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;66;03m#for the cluster this stock_id is in\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ((x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres_0\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m---> 19\u001b[0m res1 \u001b[38;5;241m=\u001b[39m \u001b[43mres\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/frame.py:10034\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, by_row, **kwargs)\u001b[0m\n\u001b[1;32m  10022\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[1;32m  10024\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[1;32m  10025\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  10026\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10032\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m  10033\u001b[0m )\n\u001b[0;32m> 10034\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/apply.py:837\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[1;32m    835\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/apply.py:963\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    962\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 963\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[1;32m    966\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/apply.py:979\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[1;32m    978\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 979\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    981\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    982\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    983\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[23], line 13\u001b[0m, in \u001b[0;36mweight_func\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mweight_func\u001b[39m(x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i,cluster \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mclusters\u001b[49m):\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mstock_id \u001b[38;5;129;01min\u001b[39;00m cluster:\n\u001b[1;32m     15\u001b[0m             \u001b[38;5;66;03m#in this case take the average of the first model and the one that was trained\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             \u001b[38;5;66;03m#for the cluster this stock_id is in\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m ((x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mres_0\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clusters' is not defined"
     ]
    }
   ],
   "source": [
    "res=df_valid['stock_id'].copy().to_frame()\n",
    "#create a place for the results to go\n",
    "res['final_res']=np.NaN\n",
    "\n",
    "    \n",
    "#do predictions\n",
    "for i,mod in enumerate(models):\n",
    "    res[f'res_{i}']=mod.predict(df_valid)\n",
    "\n",
    "# chooses output from the model trained \n",
    "# on the cluster that stock_id is in\n",
    "def weight_func(x):\n",
    "    for i,cluster in enumerate(clusters):\n",
    "        if x.stock_id in cluster:\n",
    "            #in this case take the average of the first model and the one that was trained\n",
    "            #for the cluster this stock_id is in\n",
    "            return ((x['res_0']))\n",
    "        \n",
    "res1 = res.apply(weight_func,axis=1)a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b9c15e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.850818017031716"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using all the models\n",
    "mean_absolute_error(res1, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "53d8082f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.8419250185785625"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using just the first trained on all the data\n",
    "mean_absolute_error(res.res_0, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41016a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#so the individual models idea is bad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b75962-cc29-4374-b949-c567cc827d4a",
   "metadata": {},
   "source": [
    "#  Run with Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d18b2dce-f7b1-4268-a148-43427cc17896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup_dataframes() \n",
    "global_stock_id_feats = get_global_stock_id_feats(df)\n",
    "# cleanup(df)\n",
    "# GetMemUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0b515b-b6f6-4955-a24c-ec41c657150e",
   "metadata": {},
   "source": [
    "# Mock API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "17ceed17-2e4b-4985-9b44-774333d32285",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.public_timeseries_testing_util import MockApi\n",
    "def make_env():\n",
    "    return MockApi()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527e3b9a",
   "metadata": {},
   "source": [
    "# Some visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3eac55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# #average kurtosis\n",
    "# df.columns\n",
    "# # for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "# #         df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "# #         df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "# plt.figure(figsize = (20, 10), dpi = 300)\n",
    "\n",
    "# sns.lineplot(data = train, x = 'date_id', y = 'target', hue = 'imbalance_buy_sell_flag', errorbar = None, palette = 'viridis')\n",
    "\n",
    "# plt.title('Average Target Over Days', weight = 'bold', fontsize = 30)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5131e4-280d-47fd-a717-067bfe81201c",
   "metadata": {},
   "source": [
    "# Real Kaggle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "242de961-7228-402b-bead-693a970d18dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating cache, test starts at date=478\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n",
      "done 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0     -1.395904\n",
       "1      0.544403\n",
       "2      0.785570\n",
       "3     -1.152986\n",
       "4     -1.342191\n",
       "         ...   \n",
       "195   -2.244198\n",
       "196   -2.237867\n",
       "197   -0.210165\n",
       "198    1.737057\n",
       "199   -3.381692\n",
       "Name: target, Length: 200, dtype: float64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "mock_api=True\n",
    "if mock_api:\n",
    "    env = make_env()\n",
    "    iter_test = env.iter_test()\n",
    "else:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "cache_size=55000 #5 days worth of data\n",
    "cache=None #used for Kaggle to calculate rolling features on 200 stocks\n",
    "\n",
    "def getcache(test):\n",
    "    #get all dates in orig dataframe\n",
    "    dates=df.date_id.unique()\n",
    "    \n",
    "    #get tests current date\n",
    "    date=test.iloc[-1].date_id\n",
    "    print(f'creating cache, test starts at date={date}')\n",
    "\n",
    "    if (date in dates):\n",
    "        i=np.where(dates == date)[0]\n",
    "        prevdate=i-1\n",
    "        cache=df.loc[df['date_id']==dates[prevdate[0]],:][-cache_size:]\n",
    "    else:\n",
    "        cache=df[-cache_size:]\n",
    "        \n",
    "    #get rid of extra columns in cache\n",
    "    dropcols=[c for c in cache.columns if c not in test.columns]\n",
    "    cache.drop(columns=dropcols, inplace=True)\n",
    "\n",
    "    return cache\n",
    "\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    \n",
    "#    I got this idea from https://github.com/gotoConversion/goto_conversion/\n",
    "    \n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    \n",
    "    return out\n",
    "\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "    # test.drop(columns=['currently_scored'],inplace=True)\n",
    "    \n",
    "    #add to the cache\n",
    "    if cache is None:\n",
    "        cache=getcache(test)\n",
    "                \n",
    "    cache=pd.concat([cache,test])\n",
    "    \n",
    "    feat = gaf.generate_all_features(cache)\n",
    "    # print(f'feat.near_price.isnull().sum()={feat.near_price.isnull().sum()}')\n",
    "    # x=model.predict(feat[-len(test):])\n",
    "    # print(type(x))\n",
    "    # print(x.shape)\n",
    "\n",
    "    #create a place for the results to go\n",
    "    # res=test.stock_id.copy().to_frame();\n",
    "    # res['final_res']=np.NaN\n",
    "    \n",
    "    # #do predictions\n",
    "    # for i,mod in enumerate(models):\n",
    "    #     res[f'res_{i}']=mod.predict(feat[-len(test):])\n",
    "    \n",
    "    # # chooses output from the model trained \n",
    "    # # on the cluster that stock_id is in\n",
    "    # def weight_func(x):\n",
    "    #     for i,cluster in enumerate(clusters):\n",
    "    #         if x.stock_id in cluster:\n",
    "    #             #in this case take the average of the first model and the one that was trained\n",
    "    #             #for the cluster this stock_id is in\n",
    "    #             return ((x['res_0']+x[f'res_{i}'])/2)\n",
    "\n",
    "    # sample_prediction['target'] = res.apply(weight_func,axis=1)\n",
    "    # sample_prediction['target'] = zero_sum(sample_prediction['target'], test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "    # env.predict(sample_prediction)\n",
    "\n",
    "    sample_prediction['target'] = model.predict(feat[-len(test):])\n",
    "    sample_prediction['target'] = zero_sum(sample_prediction['target'], test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "    env.predict(sample_prediction)\n",
    "\n",
    "    #just save the last part of the cache\n",
    "    cache=cache[-cache_size:]\n",
    "    print('done 1')\n",
    "\n",
    "sample_prediction['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41de171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No traceback has been produced, nothing to debug.\n"
     ]
    }
   ],
   "source": [
    "%debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35dc5613-e48c-4452-86b8-03a1b592e8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def zero_sum(prices, volumes):\n",
    "#     std_error = np.sqrt(volumes)\n",
    "#     step = np.sum(prices) / np.sum(std_error)\n",
    "#     out = prices - std_error * step\n",
    "#     return out\n",
    "\n",
    "# if is_infer:\n",
    "#     import optiver2023\n",
    "#     env = optiver2023.make_env()\n",
    "#     iter_test = env.iter_test()\n",
    "#     counter = 0\n",
    "#     y_min, y_max = -64, 64\n",
    "#     qps, predictions = [], []\n",
    "#     cache = pd.DataFrame()\n",
    "\n",
    "#     # Weights for each fold model\n",
    "#     model_weights = [1/len(models)] * len(models) \n",
    "    \n",
    "#     for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "#         now_time = time.time()\n",
    "#         cache = pd.concat([cache, test], ignore_index=True, axis=0)\n",
    "#         if counter > 0:\n",
    "#             cache = cache.groupby(['stock_id']).tail(21).sort_values(by=['date_id', 'seconds_in_bucket', 'stock_id']).reset_index(drop=True)\n",
    "#         feat = generate_all_features(cache)[-len(test):]\n",
    "\n",
    "#         # added after new API, reference: https://www.kaggle.com/competitions/optiver-trading-at-the-close/discussion/455690#2526672\n",
    "#         if test.currently_scored.iloc[0]== False:\n",
    "#             sample_prediction['target'] = 0\n",
    "#             env.predict(sample_prediction)\n",
    "#             counter += 1\n",
    "#             qps.append(time.time() - now_time)\n",
    "#             if counter % 10 == 0:\n",
    "#                 print(counter, 'qps:', np.mean(qps))\n",
    "#             continue\n",
    "\n",
    "#         feat = feat.drop(columns = [\"currently_scored\"])    \n",
    "#         # end of new codes for new API\n",
    "        \n",
    "#         # Generate predictions for each model and calculate the weighted average\n",
    "#         lgb_predictions = np.zeros(len(test))\n",
    "#         for model, weight in zip(models, model_weights):\n",
    "#             lgb_predictions += weight * model.predict(feat)\n",
    "\n",
    "#         lgb_predictions = zero_sum(lgb_predictions, test['bid_size'] + test['ask_size'])\n",
    "#         clipped_predictions = np.clip(lgb_predictions, y_min, y_max)\n",
    "#         sample_prediction['target'] = clipped_predictions\n",
    "#         env.predict(sample_prediction)\n",
    "#         counter += 1\n",
    "#         qps.append(time.time() - now_time)\n",
    "#         if counter % 10 == 0:\n",
    "#             print(counter, 'qps:', np.mean(qps))\n",
    "\n",
    "#     time_cost = 1.146 * np.mean(qps)\n",
    "#     print(f\"The code will take approximately {np.round(time_cost, 4)} hours to reason about\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab-debugger2",
   "language": "python",
   "name": "jupyterlab-debugger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
