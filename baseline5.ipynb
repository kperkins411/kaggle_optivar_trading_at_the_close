{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71f76d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:38.296752Z",
     "iopub.status.busy": "2023-12-19T14:26:38.296411Z",
     "iopub.status.idle": "2023-12-19T14:26:41.460025Z",
     "shell.execute_reply": "2023-12-19T14:26:41.459023Z"
    },
    "papermill": {
     "duration": 3.173952,
     "end_time": "2023-12-19T14:26:41.462327",
     "exception": false,
     "start_time": "2023-12-19T14:26:38.288375",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc  \n",
    "import os  \n",
    "import time  \n",
    "import warnings \n",
    "from itertools import combinations  \n",
    "from warnings import simplefilter \n",
    "import joblib  \n",
    "# import playground.optivarfuncs as of\n",
    "import lightgbm as lgb  \n",
    "import numpy as np  \n",
    "import pandas as pd  \n",
    "from sklearn.metrics import mean_absolute_error \n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit  \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "is_offline = False \n",
    "is_train = True  \n",
    "is_infer = True \n",
    "max_lookback = np.nan \n",
    "split_day = 435  \n",
    "import polars as pl\n",
    "from gc import collect;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f68695b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:41.816741Z",
     "iopub.status.busy": "2023-12-19T14:26:41.816417Z",
     "iopub.status.idle": "2023-12-19T14:26:41.844671Z",
     "shell.execute_reply": "2023-12-19T14:26:41.843753Z"
    },
    "papermill": {
     "duration": 0.037704,
     "end_time": "2023-12-19T14:26:41.846899",
     "exception": false,
     "start_time": "2023-12-19T14:26:41.809195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "#--------------------------------------get splits (no unit tests)\n",
    "import numpy as np\n",
    "\n",
    "def getsplitday( X, val_size, verbose = True):\n",
    "    '''\n",
    "    find split that seperates by day\n",
    "    ex \n",
    "    tstdf=of.getdf(numb_days= 10, drop_days= [2,3])\n",
    "    getsplitday( df, val_sze=0.1)\n",
    "    9\n",
    "    '''\n",
    "    days=X.date_id.unique()\n",
    "    index=int(np.floor(len(days)*(1-val_size)))\n",
    "    if (verbose):\n",
    "        print(f\"For days{days} and val_size={val_size}, split day={days[index]}\")\n",
    "    return days[index]\n",
    "\n",
    "def splitDataset(X, val_size, copy=False,verbose=True):\n",
    "    '''\n",
    "    ensures datasets do not split accross a day\n",
    "    returns train and val\n",
    "    ex \n",
    "    tstdf=of.getdf(numb_days= 10, drop_days= [2,3])\n",
    "    X_train, X_val = splitDataset(tstdf, 0.2, verbose=False):\n",
    "    '''\n",
    "    #get the day to split on\n",
    "    day=getsplitday( X,val_size, verbose)\n",
    "    # print(f'df shape={X.shape}, min_date={X.date_id.min()},max_date={X.date_id.max()}, val_start={val_start}, tst_start={tst_start}')\n",
    "\n",
    "    #get train, val\n",
    "    if(copy==True):\n",
    "        X_train=X[X.eval(f\"(date_id<{day})\")].copy()\n",
    "        X_val=X.loc[(X.date_id>=day)].copy()\n",
    "    else:\n",
    "        X_train=X[X.eval(f\"(date_id<{day})\")]\n",
    "        X_val=X.loc[(X.date_id>=day)]\n",
    "    return  X_train, X_val\n",
    "\n",
    "def splitTarget(X, dep_var='target',verbose=True):\n",
    "    '''\n",
    "    split target from X\n",
    "    ex\n",
    "    X_train, y_train = splitTarget(X_train, dep_var)\n",
    "    '''\n",
    "    y = X[dep_var]\n",
    "    X.drop(columns=[dep_var],inplace=True)\n",
    "    return X,y\n",
    "\n",
    "def get2_DatasetAndTarget(X, dep_var='target', val_size=0.2,copy=False, verbose=True):\n",
    "    '''\n",
    "    split into 2 datasets\n",
    "    ex \n",
    "    tstdf=of.getdf(numb_days= 10, drop_days= [2,3])\n",
    "    X_train, X_val, y_train, y_val = get2_DatasetAndTarget(tstdf,dep_var='target', val_size, verbose=False):\n",
    "    '''\n",
    "    X_train, X_val = splitDataset(X, val_size, copy,verbose)\n",
    "    X_train, y_train = splitTarget(X_train, dep_var)\n",
    "    X_val, y_val = splitTarget(X_val, dep_var)\n",
    "    if (verbose):\n",
    "        print(f\"len(X_train)={len(X_train)} and len(X_val)={len(X_val)}\")\n",
    "        tots=len(X_val)+len(X_train)\n",
    "        print(f\"train size={len(X_train)/tots}, val size={len(X_val)/tots}\")\n",
    "\n",
    "    return X_train, X_val, y_train, y_val\n",
    "\n",
    "def get3_DatasetAndTarget(X, dep_var='target', val_size=0.1, test_size=0.2,copy=False,verbose = True):\n",
    "    '''\n",
    "    split into 3 datasets\n",
    "    ex \n",
    "    tstdf=of.getdf(numb_days= 10, drop_days= [2,3])\n",
    "    X_train, X_val, X_tst, y_train, y_val, y_tst = get3_DatasetAndTarget(tstdf, dep_var='target', .1,.2 verbose=False):\n",
    "    '''\n",
    " \n",
    "    X_train, X_val= splitDataset(X, val_size+test_size,copy, verbose)\n",
    "    X_val, X_tst = splitDataset(X_val, test_size/(val_size+test_size),copy, verbose)\n",
    "\n",
    "    X_train, y_train = splitTarget(X_train, dep_var)\n",
    "    X_val, y_val = splitTarget(X_val, dep_var)\n",
    "    X_tst, y_tst = splitTarget(X_tst, dep_var)\n",
    "\n",
    "    if (verbose):\n",
    "        print(f\"len(X_train)={len(X_train)} and len(X_val)={len(X_val)}, len(X_tst)={len(X_tst)}\")\n",
    "        tots=len(X_val)+len(X_train)+len(X_tst)\n",
    "        print(f\"train size={len(X_train)/tots}, val size={len(X_val)/tots}, tst size={len(X_tst)/tots}\")\n",
    "\n",
    "    return X_train, X_val, X_tst, y_train, y_val, y_tst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d697a5b0",
   "metadata": {
    "papermill": {
     "duration": 0.005375,
     "end_time": "2023-12-19T14:26:41.858285",
     "exception": false,
     "start_time": "2023-12-19T14:26:41.852910",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f07dcd7",
   "metadata": {
    "papermill": {
     "duration": 0.005392,
     "end_time": "2023-12-19T14:26:41.869370",
     "exception": false,
     "start_time": "2023-12-19T14:26:41.863978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Settings and helper Functions\n",
    "There are 480 dates, 5 days a week or 96 weeks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "594c8a89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:41.882660Z",
     "iopub.status.busy": "2023-12-19T14:26:41.881835Z",
     "iopub.status.idle": "2023-12-19T14:26:41.886131Z",
     "shell.execute_reply": "2023-12-19T14:26:41.885301Z"
    },
    "papermill": {
     "duration": 0.012936,
     "end_time": "2023-12-19T14:26:41.887829",
     "exception": false,
     "start_time": "2023-12-19T14:26:41.874893",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:    \n",
    "    runOnKaggle=False #if true, then concat all datasets before calculating features for Kaggle data\n",
    "    #just a week for testing?\n",
    "    start_date=475\n",
    "    \n",
    "    #take last 1 months worth? or roughly 4*5=20.  So we want from (480-20) to 480\n",
    "    doTrainModel= True #if true, #need train and test sets\n",
    "    runOnKaggle=False #if true, then concat all datasets before calculating features for Kaggle data\n",
    "\n",
    "    use_subset_of_data=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6df28333",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:42.044054Z",
     "iopub.status.busy": "2023-12-19T14:26:42.043705Z",
     "iopub.status.idle": "2023-12-19T14:26:42.053840Z",
     "shell.execute_reply": "2023-12-19T14:26:42.052430Z"
    },
    "papermill": {
     "duration": 0.019878,
     "end_time": "2023-12-19T14:26:42.055682",
     "exception": false,
     "start_time": "2023-12-19T14:26:42.035804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAM usage = 3.642 GB'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tracking kernel memory usage:-  \n",
    "from os import path, walk, getpid;\n",
    "from psutil import Process;\n",
    "def GetMemUsage():\n",
    "    \"\"\"\n",
    "    This function defines the memory usage across the kernel. \n",
    "    Source-\n",
    "    https://stackoverflow.com/questions/61366458/how-to-find-memory-usage-of-kaggle-notebook\n",
    "    \"\"\";\n",
    "    \n",
    "    pid = getpid();\n",
    "    py = Process(pid);\n",
    "    memory_use = py.memory_info()[0] / 2. ** 30;\n",
    "    return f\"RAM usage = {memory_use :.4} GB\";\n",
    "\n",
    "def cleanup(df):\n",
    "    try:\n",
    "        del df\n",
    "        df=None\n",
    "    except:\n",
    "        pass\n",
    "    collect()\n",
    "    return GetMemUsage()\n",
    "\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "95f2813f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:42.069824Z",
     "iopub.status.busy": "2023-12-19T14:26:42.069522Z",
     "iopub.status.idle": "2023-12-19T14:26:42.080703Z",
     "shell.execute_reply": "2023-12-19T14:26:42.079502Z"
    },
    "papermill": {
     "duration": 0.02093,
     "end_time": "2023-12-19T14:26:42.082989",
     "exception": false,
     "start_time": "2023-12-19T14:26:42.062059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#logging\n",
    "import logging\n",
    "# set up logging to file - see previous section for more details\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    filename='logg.log',\n",
    "                    filemode='w')\n",
    "# define a Handler which writes INFO messages or higher to the sys.stderr\n",
    "console = logging.StreamHandler()\n",
    "# add the handler to the root logger\n",
    "logging.getLogger().addHandler(console)\n",
    "logger=logging.getLogger()\n",
    "\n",
    "#use following to enable and disable\n",
    "# logger.disabled = True\n",
    "\n",
    "def reduce_mem_usage(df, verbose=True):\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            \n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)\n",
    "            else:\n",
    "               \n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    if verbose:\n",
    "        logger.info(f\"Memory usage of dataframe is {start_mem:.2f} MB\")\n",
    "        end_mem = df.memory_usage().sum() / 1024**2\n",
    "        logger.info(f\"Memory usage after optimization is: {end_mem:.2f} MB\")\n",
    "        decrease = 100 * (start_mem - end_mem) / start_mem\n",
    "        logger.info(f\"Decreased by {decrease:.2f}%\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9592de95",
   "metadata": {
    "papermill": {
     "duration": 0.005674,
     "end_time": "2023-12-19T14:26:42.095033",
     "exception": false,
     "start_time": "2023-12-19T14:26:42.089359",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Parallel Triplet Imbalance Calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1bb3b375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:42.108573Z",
     "iopub.status.busy": "2023-12-19T14:26:42.107801Z",
     "iopub.status.idle": "2023-12-19T14:26:43.436305Z",
     "shell.execute_reply": "2023-12-19T14:26:43.435565Z"
    },
    "papermill": {
     "duration": 1.337792,
     "end_time": "2023-12-19T14:26:43.438479",
     "exception": false,
     "start_time": "2023-12-19T14:26:42.100687",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import njit, prange\n",
    "\n",
    "@njit(parallel=True)\n",
    "def compute_triplet_imbalance(df_values, comb_indices):\n",
    "    num_rows = df_values.shape[0]\n",
    "    num_combinations = len(comb_indices)\n",
    "    imbalance_features = np.empty((num_rows, num_combinations))\n",
    "    for i in prange(num_combinations):\n",
    "        a, b, c = comb_indices[i]\n",
    "        for j in range(num_rows):\n",
    "            max_val = max(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            min_val = min(df_values[j, a], df_values[j, b], df_values[j, c])\n",
    "            mid_val = df_values[j, a] + df_values[j, b] + df_values[j, c] - min_val - max_val\n",
    "            \n",
    "            if mid_val == min_val:\n",
    "                imbalance_features[j, i] = np.nan\n",
    "            else:\n",
    "                imbalance_features[j, i] = (max_val - mid_val) / (mid_val - min_val)\n",
    "\n",
    "    return imbalance_features\n",
    "\n",
    "def calculate_triplet_imbalance_numba(price, df):\n",
    "    df_values = df[price].values\n",
    "    comb_indices = [(price.index(a), price.index(b), price.index(c)) for a, b, c in combinations(price, 3)]\n",
    "    features_array = compute_triplet_imbalance(df_values, comb_indices)\n",
    "    columns = [f\"{a}_{b}_{c}_imb2\" for a, b, c in combinations(price, 3)]\n",
    "    features = pd.DataFrame(features_array, columns=columns)\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169e49c5",
   "metadata": {
    "papermill": {
     "duration": 0.005856,
     "end_time": "2023-12-19T14:26:43.451226",
     "exception": false,
     "start_time": "2023-12-19T14:26:43.445370",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Feature Generation Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b8f18203",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:43.464873Z",
     "iopub.status.busy": "2023-12-19T14:26:43.464328Z",
     "iopub.status.idle": "2023-12-19T14:26:43.473075Z",
     "shell.execute_reply": "2023-12-19T14:26:43.472241Z"
    },
    "papermill": {
     "duration": 0.018346,
     "end_time": "2023-12-19T14:26:43.475531",
     "exception": false,
     "start_time": "2023-12-19T14:26:43.457185",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# from tqdm.auto import tqdm  # for notebooks\n",
    "\n",
    "# Create new `pandas` methods which use `tqdm` progress\n",
    "# (can use tqdm_gui, optional kwargs, etc.)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "027807fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for backfilling\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class LastValueAllStocks:\n",
    "    numberstocks = 200\n",
    "    date_id = 0 #initialise to 0\n",
    "    all_near_prices = {i:0 for i in range(numberstocks)}\n",
    "    all_far_prices = {i:0 for i in range(numberstocks)}\n",
    "\n",
    "    def set_initial_prices(self,df):\n",
    "        #Set the initial near_and far price for the first entry for each stock in dataframe\n",
    "\n",
    "        #first get the very first entry for all stocks\n",
    "        earliest_date=df.date_id.min()\n",
    "        earliest_seconds_in_bucket=df[df.date_id==earliest_date].seconds_in_bucket.min()\n",
    "        indices=df.loc[((df.date_id==earliest_date) & (df.seconds_in_bucket==earliest_seconds_in_bucket)),:].index\n",
    " \n",
    "        #then loop through and set the near and far prices for each stock in dftmp\n",
    "        for index in indices:\n",
    "            stk=df[df.index==index].stock_id.values[0]       \n",
    "            df.loc[df.index==index,['near_price']]=self.all_near_prices[stk]\n",
    "            df.loc[df.index==index,['far_price']]=self.all_far_prices[stk]\n",
    "        return df\n",
    "\n",
    "    def save_last_prices(self,df):\n",
    "        #save the final values for near_and far price for each stock in df\n",
    "\n",
    "        #first get the very last entry for all stocks\n",
    "        latest_date=df.date_id.max()\n",
    "        latest_seconds_in_bucket=df[df.date_id==latest_date].seconds_in_bucket.max()\n",
    "        indices=df.loc[((df.date_id==latest_date) & (df.seconds_in_bucket==latest_seconds_in_bucket)),:].index\n",
    "\n",
    "        #then loop through and set the near and far prices for each stock in dftmp\n",
    "        for index in indices:\n",
    "            stk=df[df.index==index].stock_id.values[0] \n",
    "            self.all_near_prices[stk]= df.loc[df.index==index,['near_price']].values[0][0] \n",
    "            self.all_far_prices[stk]= df.loc[df.index==index,['far_price']].values[0][0]\n",
    "        \n",
    "class HandleNaNs:\n",
    "    def __init__(self):\n",
    "        self.last_value_all_stocks=LastValueAllStocks()\n",
    "        \n",
    "    def fill_nans(self,df):\n",
    "\n",
    "            #sort to get stock ids all together, should be chunks of 200 rows\n",
    "            df.sort_values(by=['stock_id','date_id','seconds_in_bucket'],inplace=True)\n",
    "\n",
    "            df=self.last_value_all_stocks.set_initial_prices(df)\n",
    "            \n",
    "            #then do a forward interpolation\n",
    "            df.far_price=df.far_price.interpolate(method='linear')\n",
    "            df.near_price=df.near_price.interpolate(method='linear')\n",
    "\n",
    "            self.last_value_all_stocks.save_last_prices(df)\n",
    "            df.sort_values(by=['date_id','seconds_in_bucket','stock_id'],inplace=True)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a135039b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:43.489780Z",
     "iopub.status.busy": "2023-12-19T14:26:43.489444Z",
     "iopub.status.idle": "2023-12-19T14:26:43.514235Z",
     "shell.execute_reply": "2023-12-19T14:26:43.513015Z"
    },
    "papermill": {
     "duration": 0.034409,
     "end_time": "2023-12-19T14:26:43.516266",
     "exception": false,
     "start_time": "2023-12-19T14:26:43.481857",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def imbalance_features(df):\n",
    "    # Define lists of price and size-related column names\n",
    "    prices = [\"reference_price\", \"far_price\", \"near_price\", \"ask_price\", \"bid_price\", \"wap\"]\n",
    "    sizes = [\"matched_size\", \"bid_size\", \"ask_size\", \"imbalance_size\"]\n",
    "    \n",
    "    df[\"volume\"] = df.eval(\"ask_size + bid_size\")\n",
    "    df[\"mid_price\"] = df.eval(\"(ask_price + bid_price) / 2\")\n",
    "    df[\"liquidity_imbalance\"] = df.eval(\"(bid_size-ask_size)/(bid_size+ask_size)\")\n",
    "    df[\"matched_imbalance\"] = df.eval(\"(imbalance_size-matched_size)/(matched_size+imbalance_size)\")\n",
    "    df[\"size_imbalance\"] = df.eval(\"bid_size / ask_size\")\n",
    "\n",
    "    for c in combinations(prices, 2):\n",
    "        df[f\"{c[0]}_{c[1]}_imb\"] = df.eval(f\"({c[0]} - {c[1]})/({c[0]} + {c[1]})\")\n",
    "\n",
    "    for c in [['ask_price', 'bid_price', 'wap', 'reference_price'], sizes]:\n",
    "        triplet_feature = calculate_triplet_imbalance_numba(c, df)\n",
    "        df[triplet_feature.columns] = triplet_feature.values\n",
    "    \n",
    "    df[\"stock_weights\"] = df[\"stock_id\"].map(weights)\n",
    "    df[\"weighted_wap\"] = df[\"stock_weights\"] * df[\"wap\"]\n",
    "    df['wap_momentum'] = df.groupby('stock_id')['weighted_wap'].pct_change(periods=6)\n",
    "   \n",
    "    df[\"imbalance_momentum\"] = df.groupby(['stock_id'])['imbalance_size'].diff(periods=1) / df['matched_size']\n",
    "    df[\"price_spread\"] = df[\"ask_price\"] - df[\"bid_price\"]\n",
    "    df[\"spread_intensity\"] = df.groupby(['stock_id'])['price_spread'].diff()\n",
    "    df['price_pressure'] = df['imbalance_size'] * (df['ask_price'] - df['bid_price'])\n",
    "    df['market_urgency'] = df['price_spread'] * df['liquidity_imbalance']\n",
    "    df['depth_pressure'] = (df['ask_size'] - df['bid_size']) * (df['far_price'] - df['near_price'])\n",
    "    \n",
    "    df['spread_depth_ratio'] = (df['ask_price'] - df['bid_price']) / (df['bid_size'] + df['ask_size'])\n",
    "    df['mid_price_movement'] = df['mid_price'].diff(periods=5).apply(lambda x: 1 if x > 0 else (-1 if x < 0 else 0))\n",
    "    \n",
    "    df['micro_price'] = ((df['bid_price'] * df['ask_size']) + (df['ask_price'] * df['bid_size'])) / (df['bid_size'] + df['ask_size'])\n",
    "    df['relative_spread'] = (df['ask_price'] - df['bid_price']) / df['wap']\n",
    "    \n",
    "    #TODO anyway to save the whole?\n",
    "    # Calculate various statistical aggregation features\n",
    "    for func in [\"mean\", \"std\", \"skew\", \"kurt\"]:\n",
    "        df[f\"all_prices_{func}\"] = df[prices].agg(func, axis=1)\n",
    "        df[f\"all_sizes_{func}\"] = df[sizes].agg(func, axis=1)\n",
    "        \n",
    "\n",
    "    for col in ['matched_size', 'imbalance_size', 'reference_price', 'imbalance_buy_sell_flag']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_shift_{window}\"] = df.groupby('stock_id')[col].shift(window)\n",
    "            df[f\"{col}_ret_{window}\"] = df.groupby('stock_id')[col].pct_change(window)\n",
    "    \n",
    "    # Calculate diff features for specific columns\n",
    "    for col in ['ask_price', 'bid_price', 'ask_size', 'bid_size', 'weighted_wap','price_spread']:\n",
    "        for window in [1,3,5,10]:\n",
    "            df[f\"{col}_diff_{window}\"] = df.groupby(\"stock_id\")[col].diff(window)\n",
    "    \n",
    "    #V4 feature\n",
    "    for window in [3,5,10]:\n",
    "        df[f'price_change_diff_{window}'] = df[f'bid_price_diff_{window}'] - df[f'ask_price_diff_{window}']\n",
    "        df[f'size_change_diff_{window}'] = df[f'bid_size_diff_{window}'] - df[f'ask_size_diff_{window}']\n",
    "\n",
    "    #V5 - rolling diff\n",
    "    # Convert from pandas to Polars\n",
    "    pl_df = pl.from_pandas(df)\n",
    "\n",
    "    #Define the windows and columns for which you want to calculate the rolling statistics\n",
    "    windows = [3, 5, 10]\n",
    "    columns = ['ask_price', 'bid_price', 'ask_size', 'bid_size']\n",
    "\n",
    "    # prepare the operations for each column and window\n",
    "    group = [\"stock_id\"]\n",
    "    expressions = []\n",
    "\n",
    "    # Loop over each window and column to create the rolling mean and std expressions\n",
    "    for window in windows:\n",
    "        for col in columns:\n",
    "            rolling_mean_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_mean(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            rolling_std_expr = (\n",
    "                pl.col(f\"{col}_diff_{window}\")\n",
    "                .rolling_std(window)\n",
    "                .over(group)\n",
    "                .alias(f'rolling_std_diff_{col}_{window}')\n",
    "            )\n",
    "\n",
    "            expressions.append(rolling_mean_expr)\n",
    "            expressions.append(rolling_std_expr)\n",
    "\n",
    "    # Run the operations using Polars' lazy API\n",
    "    lazy_df = pl_df.lazy().with_columns(expressions)\n",
    "\n",
    "    # Execute the lazy expressions and overwrite the pl_df variable\n",
    "    pl_df = lazy_df.collect()\n",
    "\n",
    "    # Convert back to pandas if necessary\n",
    "    df = pl_df.to_pandas()\n",
    "    gc.collect()\n",
    "    \n",
    "    df['mid_price*volume'] = df['mid_price_movement'] * df['volume']\n",
    "    df['harmonic_imbalance'] = df.eval('2 / ((1 / bid_size) + (1 / ask_size))')\n",
    "    \n",
    "    for col in df.columns:\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "    return df\n",
    "\n",
    "def other_features(df):\n",
    "    df[\"dow\"] = df[\"date_id\"] % 5  # Day of the week\n",
    "    df[\"seconds\"] = df[\"seconds_in_bucket\"] % 60  \n",
    "    df[\"minute\"] = df[\"seconds_in_bucket\"] // 60  \n",
    "    df['time_to_market_close'] = 540 - df['seconds_in_bucket']\n",
    "    \n",
    "    for key, value in global_stock_id_feats.items():\n",
    "        df[f\"global_{key}\"] = df[\"stock_id\"].map(value.to_dict())\n",
    "\n",
    "    return df\n",
    "\n",
    "class gen_all_features():\n",
    "    def __init__(self,df=None):\n",
    "        #infer near and far prices\n",
    "        self.hn=HandleNaNs()\n",
    "        \n",
    "    def generate_all_features(self,df):\n",
    "        #infer near and far prices\n",
    "\n",
    "        df=self.hn.fill_nans(df)\n",
    "        \n",
    "        # Select relevant columns for feature generation\n",
    "        cols = [c for c in df.columns if c not in [\"row_id\", \"time_id\"]]\n",
    "        df = df[cols]\n",
    "        \n",
    "        # Generate imbalance features\n",
    "        df = imbalance_features(df)\n",
    "        gc.collect() \n",
    "        df = other_features(df)\n",
    "        gc.collect()  \n",
    "        feature_name = [i for i in df.columns if i not in [\"row_id\", \"time_id\"]]\n",
    "        \n",
    "        return df[feature_name]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61115d61",
   "metadata": {
    "papermill": {
     "duration": 0.005747,
     "end_time": "2023-12-19T14:26:43.528228",
     "exception": false,
     "start_time": "2023-12-19T14:26:43.522481",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Loading and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "246baf17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:43.542320Z",
     "iopub.status.busy": "2023-12-19T14:26:43.541767Z",
     "iopub.status.idle": "2023-12-19T14:26:59.448331Z",
     "shell.execute_reply": "2023-12-19T14:26:59.447135Z"
    },
    "papermill": {
     "duration": 15.916149,
     "end_time": "2023-12-19T14:26:59.450234",
     "exception": false,
     "start_time": "2023-12-19T14:26:43.534085",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 679.35 MB\n",
      "Memory usage of dataframe is 679.35 MB\n",
      "Memory usage of dataframe is 679.35 MB\n",
      "Memory usage of dataframe is 679.35 MB\n",
      "Memory usage after optimization is: 304.71 MB\n",
      "Memory usage after optimization is: 304.71 MB\n",
      "Memory usage after optimization is: 304.71 MB\n",
      "Memory usage after optimization is: 304.71 MB\n",
      "Decreased by 55.15%\n",
      "Decreased by 55.15%\n",
      "Decreased by 55.15%\n",
      "Decreased by 55.15%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAM usage = 4.505 GB'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getdata():\n",
    "    if(CONFIG.runOnKaggle==True):\n",
    "        df = pd.read_csv(\"/kaggle/input/optiver-trading-at-the-close/train.csv\")\n",
    "    else:\n",
    "        df = pd.read_csv(\"./data/train.csv\")\n",
    "    \n",
    "    df = df.dropna(subset=[\"target\"])  #drop all rows with NaN in target\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "    \n",
    "df=getdata()  \n",
    "df=reduce_mem_usage(df)\n",
    "GetMemUsage()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3aac8f",
   "metadata": {
    "papermill": {
     "duration": 0.006091,
     "end_time": "2023-12-19T14:26:59.462896",
     "exception": false,
     "start_time": "2023-12-19T14:26:59.456805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Calculate features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7f376097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:59.478608Z",
     "iopub.status.busy": "2023-12-19T14:26:59.478226Z",
     "iopub.status.idle": "2023-12-19T14:26:59.487754Z",
     "shell.execute_reply": "2023-12-19T14:26:59.486820Z"
    },
    "papermill": {
     "duration": 0.01923,
     "end_time": "2023-12-19T14:26:59.489632",
     "exception": false,
     "start_time": "2023-12-19T14:26:59.470402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weights = [\n",
    "    0.004, 0.001, 0.002, 0.006, 0.004, 0.004, 0.002, 0.006, 0.006, 0.002, 0.002, 0.008,\n",
    "    0.006, 0.002, 0.008, 0.006, 0.002, 0.006, 0.004, 0.002, 0.004, 0.001, 0.006, 0.004,\n",
    "    0.002, 0.002, 0.004, 0.002, 0.004, 0.004, 0.001, 0.001, 0.002, 0.002, 0.006, 0.004,\n",
    "    0.004, 0.004, 0.006, 0.002, 0.002, 0.04 , 0.002, 0.002, 0.004, 0.04 , 0.002, 0.001,\n",
    "    0.006, 0.004, 0.004, 0.006, 0.001, 0.004, 0.004, 0.002, 0.006, 0.004, 0.006, 0.004,\n",
    "    0.006, 0.004, 0.002, 0.001, 0.002, 0.004, 0.002, 0.008, 0.004, 0.004, 0.002, 0.004,\n",
    "    0.006, 0.002, 0.004, 0.004, 0.002, 0.004, 0.004, 0.004, 0.001, 0.002, 0.002, 0.008,\n",
    "    0.02 , 0.004, 0.006, 0.002, 0.02 , 0.002, 0.002, 0.006, 0.004, 0.002, 0.001, 0.02,\n",
    "    0.006, 0.001, 0.002, 0.004, 0.001, 0.002, 0.006, 0.006, 0.004, 0.006, 0.001, 0.002,\n",
    "    0.004, 0.006, 0.006, 0.001, 0.04 , 0.006, 0.002, 0.004, 0.002, 0.002, 0.006, 0.002,\n",
    "    0.002, 0.004, 0.006, 0.006, 0.002, 0.002, 0.008, 0.006, 0.004, 0.002, 0.006, 0.002,\n",
    "    0.004, 0.006, 0.002, 0.004, 0.001, 0.004, 0.002, 0.004, 0.008, 0.006, 0.008, 0.002,\n",
    "    0.004, 0.002, 0.001, 0.004, 0.004, 0.004, 0.006, 0.008, 0.004, 0.001, 0.001, 0.002,\n",
    "    0.006, 0.004, 0.001, 0.002, 0.006, 0.004, 0.006, 0.008, 0.002, 0.002, 0.004, 0.002,\n",
    "    0.04 , 0.002, 0.002, 0.004, 0.002, 0.002, 0.006, 0.02 , 0.004, 0.002, 0.006, 0.02,\n",
    "    0.001, 0.002, 0.006, 0.004, 0.006, 0.004, 0.004, 0.004, 0.004, 0.002, 0.004, 0.04,\n",
    "    0.002, 0.008, 0.002, 0.004, 0.001, 0.004, 0.006, 0.004,\n",
    "]\n",
    "weights = {int(k):v for k,v in enumerate(weights)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "beaa46e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:59.503942Z",
     "iopub.status.busy": "2023-12-19T14:26:59.503596Z",
     "iopub.status.idle": "2023-12-19T14:26:59.511470Z",
     "shell.execute_reply": "2023-12-19T14:26:59.510400Z"
    },
    "papermill": {
     "duration": 0.017511,
     "end_time": "2023-12-19T14:26:59.513665",
     "exception": false,
     "start_time": "2023-12-19T14:26:59.496154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 81 µs, sys: 3 µs, total: 84 µs\n",
      "Wall time: 169 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def get_global_stock_id_feats(df2):\n",
    "#first get the stats based on the training features\n",
    "    global_stock_id_feats = {\n",
    "            \"median_size\": df2.groupby(\"stock_id\")[\"bid_size\"].median() + df2.groupby(\"stock_id\")[\"ask_size\"].median(),\n",
    "            \"std_size\": df2.groupby(\"stock_id\")[\"bid_size\"].std() + df2.groupby(\"stock_id\")[\"ask_size\"].std(),\n",
    "            \"ptp_size\": df2.groupby(\"stock_id\")[\"bid_size\"].max() - df2.groupby(\"stock_id\")[\"bid_size\"].min(),\n",
    "            \"median_price\": df2.groupby(\"stock_id\")[\"bid_price\"].median() + df2.groupby(\"stock_id\")[\"ask_price\"].median(),\n",
    "            \"std_price\": df2.groupby(\"stock_id\")[\"bid_price\"].std() + df2.groupby(\"stock_id\")[\"ask_price\"].std(),\n",
    "            \"ptp_price\": df2.groupby(\"stock_id\")[\"bid_price\"].max() - df2.groupby(\"stock_id\")[\"ask_price\"].min(),\n",
    "        }\n",
    "    return global_stock_id_feats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "792acbf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:26:59.528133Z",
     "iopub.status.busy": "2023-12-19T14:26:59.527813Z",
     "iopub.status.idle": "2023-12-19T14:27:00.906303Z",
     "shell.execute_reply": "2023-12-19T14:27:00.904771Z"
    },
    "papermill": {
     "duration": 1.388517,
     "end_time": "2023-12-19T14:27:00.908847",
     "exception": false,
     "start_time": "2023-12-19T14:26:59.520330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 813 ms, sys: 24.1 ms, total: 837 ms\n",
      "Wall time: 842 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAM usage = 4.505 GB'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# cleanup_dataframes() \n",
    "\n",
    "#get these global feats on entire dataframe\n",
    "global_stock_id_feats = get_global_stock_id_feats(df)\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "85691f1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:27:00.925109Z",
     "iopub.status.busy": "2023-12-19T14:27:00.924671Z",
     "iopub.status.idle": "2023-12-19T14:40:35.280330Z",
     "shell.execute_reply": "2023-12-19T14:40:35.279015Z"
    },
    "papermill": {
     "duration": 814.366691,
     "end_time": "2023-12-19T14:40:35.282894",
     "exception": false,
     "start_time": "2023-12-19T14:27:00.916203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build df1 Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 3771.41 MB\n",
      "Memory usage of dataframe is 3771.41 MB\n",
      "Memory usage of dataframe is 3771.41 MB\n",
      "Memory usage of dataframe is 3771.41 MB\n",
      "Memory usage after optimization is: 3122.03 MB\n",
      "Memory usage after optimization is: 3122.03 MB\n",
      "Memory usage after optimization is: 3122.03 MB\n",
      "Memory usage after optimization is: 3122.03 MB\n",
      "Decreased by 17.22%\n",
      "Decreased by 17.22%\n",
      "Decreased by 17.22%\n",
      "Decreased by 17.22%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.2 s, sys: 4.07 s, total: 1min\n",
      "Wall time: 50.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'RAM usage = 3.883 GB'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "gaf=gen_all_features()\n",
    "df = gaf.generate_all_features(df)\n",
    "print(\"Build df1 Finished.\")\n",
    "\n",
    "df=reduce_mem_usage(df)\n",
    "\n",
    "GetMemUsage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f45998b",
   "metadata": {
    "papermill": {
     "duration": 0.339613,
     "end_time": "2023-12-19T14:40:36.065416",
     "exception": false,
     "start_time": "2023-12-19T14:40:35.725803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "87cd0297",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:40:36.741671Z",
     "iopub.status.busy": "2023-12-19T14:40:36.741275Z",
     "iopub.status.idle": "2023-12-19T14:40:36.753894Z",
     "shell.execute_reply": "2023-12-19T14:40:36.752768Z"
    },
    "papermill": {
     "duration": 0.352622,
     "end_time": "2023-12-19T14:40:36.756448",
     "exception": false,
     "start_time": "2023-12-19T14:40:36.403826",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters=[]\n",
    "#run 12\n",
    "#these come from months that were split into 4 clusters\n",
    "# c0=\"13  14  17  23  45  54  68  72  75  84  90  106  128  133  138  140  157  158  167  175  186\"\n",
    "# c0=[int(a) for a in c0.split('  ')]\n",
    "# c1=\"6  24  26  53  69  111  114  115  118  119  124  156  159  161  166  188  191  196  199\"\n",
    "# c1=[int(a) for a in c1.split('  ')]\n",
    "\n",
    "# #verify no intersection\n",
    "# print(f'intersection of c0 and c1={[a for a in c0 if a in c1]}')\n",
    "\n",
    "# #find all other stocks\n",
    "# allothers=[a for a in range(200) if a not in c0 and a not in c1]\n",
    "# clusters=[c0,c1,allothers]\n",
    "\n",
    "# #run 13\n",
    "# clusters=[[0, 1, 28, 35, 50, 120, 121, 138, 153, 155, 167, 171, 179, 181, 25, 41, 47, 80, 84, 85, 113, 117, 133, 151, 175, 178, 186, 7, 37, 43, 51, 60, 139, 148, 165, 189, 23, 195, 44, 86, 180, 68, 81, 90, 36, 191, 131],\n",
    "# [2, 8, 40, 53, 55, 74, 77, 100, 101, 102, 114, 130, 150, 166, 177, 9, 18, 20, 29, 33, 52, 56, 59, 62, 99, 107, 111, 125, 149, 152, 196, 16, 39, 46, 135, 168, 170, 15, 63, 67, 79, 104, 145, 164, 173, 71, 93, 98, 119, 19, 136, 123, 161, 115, 82],\n",
    "# [3, 14, 32, 45, 48, 54, 58, 70, 78, 89, 94, 110, 141, 157, 176, 184, 13, 105, 160, 83, 198],\n",
    "# [4, 12, 22, 24, 27, 30, 31, 65, 96, 103, 158, 199, 5, 61, 64, 73, 75, 106, 112, 129, 154, 163, 169, 116, 192, 21, 128, 132, 134],\n",
    "# [6, 17, 42, 57, 88, 109, 118, 122, 126, 147, 159, 182, 188, 193, 10, 34, 49, 66, 69, 91, 97, 137, 142, 185, 194, 26, 92, 146, 174, 108, 197, 127, 76, 124, 72, 140, 87, 183, 162],\n",
    "# [11, 38, 95, 144, 172, 187, 190, 143, 156]]\n",
    "\n",
    "# allothers=[]\n",
    "# flattenclusters=[c for cluster in clusters for c in cluster]\n",
    "# allothers=[a for a in range(200) if a not in flattenclusters]\n",
    "# if allothers:\n",
    "#     clusters.append(allothers)\n",
    "\n",
    "#get number of models\n",
    "num_models=len(clusters)+1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2610a3ec",
   "metadata": {
    "papermill": {
     "duration": 0.335287,
     "end_time": "2023-12-19T14:40:38.173224",
     "exception": false,
     "start_time": "2023-12-19T14:40:37.837937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f3e74da8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:40:38.916012Z",
     "iopub.status.busy": "2023-12-19T14:40:38.914833Z",
     "iopub.status.idle": "2023-12-19T14:40:41.187511Z",
     "shell.execute_reply": "2023-12-19T14:40:41.186429Z"
    },
    "papermill": {
     "duration": 2.683343,
     "end_time": "2023-12-19T14:40:41.190384",
     "exception": false,
     "start_time": "2023-12-19T14:40:38.507041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load the models\n",
    "if(CONFIG.runOnKaggle==True):\n",
    "    model_save_path = '../input/optivar-r13/' \n",
    "else:\n",
    "    model_save_path = './models/' \n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "            \n",
    "models=[]\n",
    "for i in range(num_models):\n",
    "    model_filename = os.path.join(model_save_path, f'm_{i}.txt')\n",
    "    model = lgb.Booster(model_file=model_filename)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "768c3a98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-19T14:40:41.861532Z",
     "iopub.status.busy": "2023-12-19T14:40:41.861144Z",
     "iopub.status.idle": "2023-12-19T14:55:38.892719Z",
     "shell.execute_reply": "2023-12-19T14:55:38.891449Z"
    },
    "papermill": {
     "duration": 897.371974,
     "end_time": "2023-12-19T14:55:38.894993",
     "exception": false,
     "start_time": "2023-12-19T14:40:41.523019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n",
      "creating cache, test starts at date=478\n",
      "feat shape=(200, 161),unique(seconds_in_bucket)=1, unique near_price=198, unique far_price=198\n",
      "for stock 199, seconds_in_bucket=0, near_price=0.9960629940032959, far_price=0.9960629940032959\n",
      "sample_prediction shape=(200, 2), columns=Index(['row_id', 'target'], dtype='object')\n",
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n",
      "feat shape=(200, 161),unique(seconds_in_bucket)=1, unique near_price=199, unique far_price=199\n",
      "for stock 199, seconds_in_bucket=10, near_price=0.9960629940032959, far_price=0.9960629940032959\n",
      "sample_prediction shape=(200, 2), columns=Index(['row_id', 'target'], dtype='object')\n",
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n",
      "feat shape=(200, 161),unique(seconds_in_bucket)=1, unique near_price=200, unique far_price=200\n",
      "for stock 199, seconds_in_bucket=20, near_price=0.9960629940032959, far_price=0.9960629940032959\n",
      "sample_prediction shape=(200, 2), columns=Index(['row_id', 'target'], dtype='object')\n",
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n",
      "feat shape=(200, 161),unique(seconds_in_bucket)=1, unique near_price=200, unique far_price=199\n",
      "for stock 199, seconds_in_bucket=30, near_price=0.9960629940032959, far_price=0.9960629940032959\n",
      "sample_prediction shape=(200, 2), columns=Index(['row_id', 'target'], dtype='object')\n",
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n",
      "feat shape=(200, 161),unique(seconds_in_bucket)=1, unique near_price=199, unique far_price=199\n",
      "for stock 199, seconds_in_bucket=40, near_price=0.9960629940032959, far_price=0.9960629940032959\n",
      "sample_prediction shape=(200, 2), columns=Index(['row_id', 'target'], dtype='object')\n",
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n",
      "feat shape=(200, 161),unique(seconds_in_bucket)=1, unique near_price=200, unique far_price=200\n",
      "for stock 199, seconds_in_bucket=50, near_price=0.9960629940032959, far_price=0.9960629940032959\n",
      "sample_prediction shape=(200, 2), columns=Index(['row_id', 'target'], dtype='object')\n",
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n",
      "feat shape=(200, 161),unique(seconds_in_bucket)=1, unique near_price=200, unique far_price=200\n",
      "for stock 199, seconds_in_bucket=60, near_price=0.9960629940032959, far_price=0.9960629940032959\n",
      "sample_prediction shape=(200, 2), columns=Index(['row_id', 'target'], dtype='object')\n",
      "shape test=(200, 15), columns=Index(['stock_id', 'date_id', 'seconds_in_bucket', 'imbalance_size',\n",
      "       'imbalance_buy_sell_flag', 'reference_price', 'matched_size',\n",
      "       'far_price', 'near_price', 'bid_price', 'bid_size', 'ask_price',\n",
      "       'ask_size', 'wap', 'row_id'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[87], line 62\u001b[0m\n\u001b[1;32m     58\u001b[0m     cache\u001b[38;5;241m=\u001b[39mgetcache(test)\n\u001b[1;32m     60\u001b[0m cache\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mconcat([cache,test])\n\u001b[0;32m---> 62\u001b[0m feat \u001b[38;5;241m=\u001b[39m \u001b[43mgaf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_all_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m feat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(test):]\u001b[38;5;241m=\u001b[39mfeat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(test):]\u001b[38;5;241m.\u001b[39mfillna(feat\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeat shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(test):]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,unique(seconds_in_bucket)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(test):]\u001b[38;5;241m.\u001b[39mseconds_in_bucket\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, unique near_price=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(test):]\u001b[38;5;241m.\u001b[39mnear_price\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, unique far_price=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeat[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mlen\u001b[39m(test):]\u001b[38;5;241m.\u001b[39mfar_price\u001b[38;5;241m.\u001b[39mnunique()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)  \n",
      "Cell \u001b[0;32mIn[79], line 127\u001b[0m, in \u001b[0;36mgen_all_features.generate_all_features\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_all_features\u001b[39m(\u001b[38;5;28mself\u001b[39m,df):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;66;03m#infer near and far prices\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     df\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill_nans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Select relevant columns for feature generation\u001b[39;00m\n\u001b[1;32m    130\u001b[0m     cols \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "Cell \u001b[0;32mIn[78], line 55\u001b[0m, in \u001b[0;36mHandleNaNs.fill_nans\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     52\u001b[0m df\u001b[38;5;241m.\u001b[39mfar_price\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mfar_price\u001b[38;5;241m.\u001b[39minterpolate(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m df\u001b[38;5;241m.\u001b[39mnear_price\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mnear_price\u001b[38;5;241m.\u001b[39minterpolate(method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_value_all_stocks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_last_prices\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m df\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseconds_in_bucket\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_id\u001b[39m\u001b[38;5;124m'\u001b[39m],inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn[78], line 38\u001b[0m, in \u001b[0;36mLastValueAllStocks.save_last_prices\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     36\u001b[0m stk\u001b[38;5;241m=\u001b[39mdf[df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m==\u001b[39mindex]\u001b[38;5;241m.\u001b[39mstock_id\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m] \n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_near_prices[stk]\u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[df\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m==\u001b[39mindex,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnear_price\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_far_prices[stk]\u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfar_price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/indexing.py:1147\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m   1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_value(\u001b[38;5;241m*\u001b[39mkey, takeable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_takeable)\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m     axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/indexing.py:1339\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take_opportunity(tup):\n\u001b[1;32m   1337\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multi_take(tup)\n\u001b[0;32m-> 1339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_tuple_same_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtup\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/indexing.py:994\u001b[0m, in \u001b[0;36m_LocationIndexer._getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    991\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_null_slice(key):\n\u001b[1;32m    992\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m retval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mretval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;66;03m# We should never have retval.ndim < self.ndim, as that should\u001b[39;00m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;66;03m#  be handled by the _getitem_lowerdim call above.\u001b[39;00m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m retval\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/indexing.py:1375\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_slice_axis(key, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m   1374\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[0;32m-> 1375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getbool_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like_indexer(key):\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;66;03m# an iterable multi-selection\u001b[39;00m\n\u001b[1;32m   1378\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(labels, MultiIndex)):\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/indexing.py:1173\u001b[0m, in \u001b[0;36m_LocationIndexer._getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1171\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(labels, key)\n\u001b[1;32m   1172\u001b[0m inds \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_take_with_is_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43minds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/generic.py:4088\u001b[0m, in \u001b[0;36mNDFrame._take_with_is_copy\u001b[0;34m(self, indices, axis)\u001b[0m\n\u001b[1;32m   4077\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   4078\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_take_with_is_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m, indices, axis: Axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   4079\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4080\u001b[0m \u001b[38;5;124;03m    Internal version of the `take` method that sets the `_is_copy`\u001b[39;00m\n\u001b[1;32m   4081\u001b[0m \u001b[38;5;124;03m    attribute to keep track of the parent dataframe (using in indexing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4086\u001b[0m \u001b[38;5;124;03m    See the docstring of `take` for full explanation of the parameters.\u001b[39;00m\n\u001b[1;32m   4087\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4088\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4089\u001b[0m     \u001b[38;5;66;03m# Maybe set copy if we didn't actually change the index.\u001b[39;00m\n\u001b[1;32m   4090\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result\u001b[38;5;241m.\u001b[39m_get_axis(axis)\u001b[38;5;241m.\u001b[39mequals(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis(axis)):\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/generic.py:4068\u001b[0m, in \u001b[0;36mNDFrame.take\u001b[0;34m(self, indices, axis, **kwargs)\u001b[0m\n\u001b[1;32m   4063\u001b[0m     \u001b[38;5;66;03m# We can get here with a slice via DataFrame.__getitem__\u001b[39;00m\n\u001b[1;32m   4064\u001b[0m     indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m   4065\u001b[0m         indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop, indices\u001b[38;5;241m.\u001b[39mstep, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp\n\u001b[1;32m   4066\u001b[0m     )\n\u001b[0;32m-> 4068\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4070\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_block_manager_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   4072\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\u001b[38;5;241m.\u001b[39m__finalize__(\n\u001b[1;32m   4074\u001b[0m     \u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtake\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4075\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/internals/managers.py:877\u001b[0m, in \u001b[0;36mBaseBlockManager.take\u001b[0;34m(self, indexer, axis, verify)\u001b[0m\n\u001b[1;32m    874\u001b[0m indexer \u001b[38;5;241m=\u001b[39m maybe_convert_indices(indexer, n, verify\u001b[38;5;241m=\u001b[39mverify)\n\u001b[1;32m    876\u001b[0m new_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m--> 877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/pandas/core/internals/managers.py:606\u001b[0m, in \u001b[0;36mBaseBlockManager.reindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[0m\n\u001b[1;32m    603\u001b[0m     bm\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bm\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreindex_indexer\u001b[39m(\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    608\u001b[0m     new_axis: Index,\n\u001b[1;32m    609\u001b[0m     indexer: npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    610\u001b[0m     axis: AxisInt,\n\u001b[1;32m    611\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    612\u001b[0m     allow_dups: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    613\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    614\u001b[0m     only_slice: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m    616\u001b[0m     use_na_proxy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    617\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    618\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;124;03m    ----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;124;03m    pandas-indexer with -1's only.\u001b[39;00m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1758\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.ThreadTracer.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/p39/lib/python3.9/site-packages/debugpy/_vendored/pydevd/_pydev_bundle/pydev_is_thread_alive.py:9\u001b[0m, in \u001b[0;36mis_thread_alive\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      6\u001b[0m _temp \u001b[38;5;241m=\u001b[39m threading\u001b[38;5;241m.\u001b[39mThread()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_is_stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 3.x has this\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_thread_alive\u001b[39m(t):\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39m_is_stopped\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_temp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_Thread__stopped\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Python 2.x has this\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mock_api=True\n",
    "if mock_api:\n",
    "    from data.public_timeseries_testing_util import MockApi\n",
    "    def make_env():\n",
    "        return MockApi()\n",
    "    env = make_env()\n",
    "    iter_test = env.iter_test()\n",
    "else:\n",
    "    import optiver2023\n",
    "    env = optiver2023.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "\n",
    "cache_size=55000 #5 days worth of data\n",
    "iter_test = env.iter_test()\n",
    "\n",
    "cache=None #used for Kaggle to calculate rolling features on 200 stocks\n",
    "\n",
    "def getcache(test):\n",
    "    #get all dates in orig dataframe\n",
    "    dates=df.date_id.unique()\n",
    "    \n",
    "    #get tests current date\n",
    "    date=test.iloc[-1].date_id\n",
    "    print(f'creating cache, test starts at date={date}')\n",
    "\n",
    "    if (date in dates):\n",
    "        i=np.where(dates == date)[0]\n",
    "        prevdate=i-1\n",
    "        cache=df.loc[df['date_id']==dates[prevdate[0]],:][-cache_size:]\n",
    "    else:\n",
    "        cache=df[-cache_size:]\n",
    "        \n",
    "    #get rid of extra columns in cache\n",
    "    dropcols=[c for c in cache.columns if c not in test.columns]\n",
    "    cache.drop(columns=dropcols, inplace=True)\n",
    "\n",
    "    return cache\n",
    "\n",
    "\n",
    "def zero_sum(prices, volumes):\n",
    "    \n",
    "#    I got this idea from https://github.com/gotoConversion/goto_conversion/\n",
    "    \n",
    "    std_error = np.sqrt(volumes)\n",
    "    step = np.sum(prices)/np.sum(std_error)\n",
    "    out = prices-std_error*step\n",
    "    \n",
    "    return out\n",
    "\n",
    "for (test, revealed_targets, sample_prediction) in iter_test:\n",
    "    print(f'shape test={test.shape}, columns={test.columns}')\n",
    "    cols=test.columns\n",
    "    if 'currently_scored' in cols:\n",
    "        test.drop(columns=['currently_scored'],inplace=True)\n",
    "    \n",
    "    #add to the cache\n",
    "    if cache is None:\n",
    "        cache=getcache(test)\n",
    "        \n",
    "    cache=pd.concat([cache,test])\n",
    "    \n",
    "    feat = gaf.generate_all_features(cache)\n",
    "    \n",
    "    feat[-len(test):]=feat[-len(test):].fillna(feat.mean())\n",
    "\n",
    "    print(f'feat shape={feat[-len(test):].shape},unique(seconds_in_bucket)={feat[-len(test):].seconds_in_bucket.nunique()}, unique near_price={feat[-len(test):].near_price.nunique()}, unique far_price={feat[-len(test):].far_price.nunique()}')  \n",
    "    # print(f'seconds_in_bucket)={feat[-len(test):].seconds_in_bucket}, unique near_price={feat[-len(test):].near_price.nunique()}, unique far_price={feat[-len(test):].far_price.nunique()}')  \n",
    "    print(f'for stock {feat[-1:].stock_id.values[0]}, seconds_in_bucket={feat[-1:].seconds_in_bucket.values[0]}, near_price={feat[-1:].near_price.values[0]}, far_price={feat[-1:].far_price.values[0]}')  \n",
    "   \n",
    "    # #create a place for the results to go\n",
    "    # res=test.stock_id.copy().to_frame();\n",
    "    # res['final_res']=np.NaN\n",
    "    \n",
    "    # #do predictions\n",
    "    # for i,mod in enumerate(models):\n",
    "    #     res[f'res_{i}']=mod.predict(feat[-len(test):])\n",
    "    \n",
    "    # # chooses output from the model trained \n",
    "    # # on the cluster that stock_id is in\n",
    "    # def weight_func(x):\n",
    "    #     for i,cluster in enumerate(clusters):\n",
    "    #         if x.stock_id in cluster:\n",
    "    #             #in this case take the average of the first model and the one that was trained\n",
    "    #             #for the cluster this stock_id is in\n",
    "    #             return ((x['res_0']+x[f'res_{i}'])/2)\n",
    "            \n",
    "    sample_prediction['target'] = model.predict(feat[-len(test):])\n",
    "    sample_prediction['target'] = zero_sum(sample_prediction['target'], feat.loc[-len(test):,'bid_size']+feat.loc[-len(test):,'ask_size'])\n",
    "    # sample_prediction['target'] = zero_sum(sample_prediction['target'], test.loc[:,'bid_size'] + test.loc[:,'ask_size'])\n",
    "\n",
    "    print(f'sample_prediction shape={sample_prediction.shape}, columns={sample_prediction.columns}')\n",
    "    env.predict(sample_prediction)\n",
    "       \n",
    "    #just save the last part of the cache\n",
    "    cache=cache[-cache_size:]\n",
    "\n",
    "sample_prediction['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8320d7",
   "metadata": {
    "papermill": {
     "duration": 0.645985,
     "end_time": "2023-12-19T14:55:40.256480",
     "exception": false,
     "start_time": "2023-12-19T14:55:39.610495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 7056235,
     "sourceId": 57891,
     "sourceType": "competition"
    },
    {
     "datasetId": 4188306,
     "sourceId": 7233035,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30626,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "jupyterlab-debugger2",
   "language": "python",
   "name": "jupyterlab-debugger2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1746.743848,
   "end_time": "2023-12-19T14:55:42.406014",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-19T14:26:35.662166",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
